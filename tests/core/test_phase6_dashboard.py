"""
Phase 6: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æç”»ã®ãƒ†ã‚¹ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
"""

import pytest
import tempfile
import shutil
from pathlib import Path
from unittest.mock import MagicMock

from prototype.local_cli.core.phase6_dashboard import (
    render_dashboard,
    DashboardError,
)
from prototype.local_cli.core.types import (
    EnvironmentConfig,
    JiraMetadata,
    BoardMetadata,
    SprintMetadata,
    CoreData,
    ParentTask,
    SubtaskData,
    TaskTotals,
    MetricsCollection,
    AISummary,
)


@pytest.fixture
def temp_output_dir():
    """ä¸€æ™‚å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
    temp_dir = tempfile.mkdtemp(prefix="test_dashboard_")
    yield Path(temp_dir)
    shutil.rmtree(temp_dir, ignore_errors=True)


@pytest.fixture
def config(temp_output_dir):
    """ç’°å¢ƒè¨­å®šã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    return EnvironmentConfig(
        jira_domain="https://test.atlassian.net",
        jira_email="test@example.com",
        jira_api_token="token123",
        output_dir=str(temp_output_dir),
        target_done_rate=0.8,
        axis_mode="percent",
        gemini_api_key="test-api-key",
        gemini_model="gemini-2.5-flash-lite",
        gemini_disable=False,
    )


@pytest.fixture
def metadata():
    """Jiraãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    board = {
        "id": 123,
        "name": "Test Board"
    }
    sprint = {
        "id": 456,
        "name": "Sprint 1",
        "state": "active"
    }
    return JiraMetadata(
        board=BoardMetadata(
            board=board,
            board_id=123,
            project_key="TEST",
            boards_count=1
        ),
        sprint=SprintMetadata(
            sprint=sprint,
            sprint_id=456,
            sprint_name="Sprint 1",
            sprint_start="2024-01-01",
            sprint_end="2024-01-14",
            active_sprints_count=1
        ),
        project_key="TEST",
        story_points_field="customfield_10016"
    )


@pytest.fixture
def core_data():
    """ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    return CoreData(
        parents=[
            ParentTask(
                key="TEST-100",
                summary="Parent Task",
                assignee="user1@example.com",
                subtasks=[
                    SubtaskData(
                        key="TEST-101",
                        summary="Subtask 1",
                        assignee="user1@example.com",
                        status="å®Œäº†",
                        done=True,
                        started_at="2024-01-02T10:00:00.000+0900",
                        completed_at="2024-01-02T12:00:00.000+0900"
                    ),
                    SubtaskData(
                        key="TEST-102",
                        summary="Subtask 2",
                        assignee="user2@example.com",
                        status="é€²è¡Œä¸­",
                        done=False,
                        started_at="2024-01-02T11:00:00.000+0900",
                        completed_at=None
                    )
                ]
            )
        ],
        totals=TaskTotals(
            subtasks=2,
            done=1,
            not_done=1
        )
    )


@pytest.fixture
def metrics():
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    return MetricsCollection(
        burndown=None,
        velocity=None,
        project_sprint_count=None,
        status_counts=None,
        time_in_status=None,
        workload=None,
        kpis={
            "overdue": 0,
            "due_soon": 0,
            "high_priority_todo": 0,
            "unassigned": 0,
        },
        risks={},
        evidence=None,
        project_subtask_count=None,
        assignee_workload={
            "user1@example.com": {"subtasks": 1, "done": 1},
            "user2@example.com": {"subtasks": 1, "done": 0}
        }
    )


@pytest.fixture
def ai_summary():
    """AIè¦ç´„ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    return AISummary(
        full_text="## ğŸ¯ çµè«–\nå®Œäº†ç‡50% - æ³¨æ„âš ï¸",
        evidence_reasons={"TEST-101": "å„ªå…ˆåº¦é«˜"}
    )


# ============================================================
# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æç”»ã®ãƒ†ã‚¹ãƒˆ
# ============================================================

def test_render_dashboard_basic(
    config,
    metadata,
    core_data,
    metrics,
    temp_output_dir
):
    """åŸºæœ¬çš„ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æç”»"""
    # ãƒ¢ãƒƒã‚¯ã®draw_pngé–¢æ•°
    mock_draw_png = MagicMock()
    
    # æç”»å®Ÿè¡Œï¼ˆé–¢æ•°ã‚’ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ãƒˆï¼‰
    output_path = render_dashboard(
        config, metadata, core_data, metrics, 
        enable_logging=True,
        _draw_png_func=mock_draw_png
    )
    
    # çµæœæ¤œè¨¼
    assert output_path == temp_output_dir / "sprint_overview.png"
    assert mock_draw_png.called
    
    # draw_pngã®å¼•æ•°ã‚’æ¤œè¨¼
    call_args = mock_draw_png.call_args
    assert call_args is not None
    assert call_args[1]["data"] == core_data.to_dict()
    assert call_args[1]["extras"] == metrics.to_dict()
    assert call_args[1]["target_done_rate"] == 0.8


def test_render_dashboard_with_ai_summary(
    config,
    metadata,
    core_data,
    metrics,
    ai_summary,
    temp_output_dir
):
    """AIè¦ç´„ä»˜ããƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æç”»"""
    # ãƒ¢ãƒƒã‚¯ã®draw_pngé–¢æ•°
    mock_draw_png = MagicMock()
    
    # æç”»å®Ÿè¡Œ
    output_path = render_dashboard(
        config, metadata, core_data, metrics, ai_summary,
        enable_logging=True,
        _draw_png_func=mock_draw_png
    )
    
    # çµæœæ¤œè¨¼
    assert output_path == temp_output_dir / "sprint_overview.png"
    assert mock_draw_png.called
    
    # AIè¦ç´„ãŒextrasã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    call_args = mock_draw_png.call_args
    extras = call_args[1]["extras"]
    assert "ai_full_text" in extras
    assert extras["ai_full_text"] == "## ğŸ¯ çµè«–\nå®Œäº†ç‡50% - æ³¨æ„âš ï¸"
    assert "ai_reasons" in extras
    assert extras["ai_reasons"] == {"TEST-101": "å„ªå…ˆåº¦é«˜"}


def test_render_dashboard_without_ai_summary(
    config,
    metadata,
    core_data,
    metrics,
    temp_output_dir
):
    """AIè¦ç´„ãªã—ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æç”»"""
    # ãƒ¢ãƒƒã‚¯ã®draw_pngé–¢æ•°
    mock_draw_png = MagicMock()
    
    # æç”»å®Ÿè¡Œ
    output_path = render_dashboard(
        config, metadata, core_data, metrics, None,
        enable_logging=False,
        _draw_png_func=mock_draw_png
    )
    
    # çµæœæ¤œè¨¼
    assert output_path == temp_output_dir / "sprint_overview.png"
    assert mock_draw_png.called
    
    # AIè¦ç´„ãŒextrasã«å«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
    call_args = mock_draw_png.call_args
    extras = call_args[1]["extras"]
    assert "ai_full_text" not in extras or extras.get("ai_full_text") is None


def test_render_dashboard_draw_png_error(
    config,
    metadata,
    core_data,
    metrics
):
    """draw_pngé–¢æ•°ãŒã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã™å ´åˆ"""
    # ãƒ¢ãƒƒã‚¯ã®draw_pngé–¢æ•°ãŒã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã™
    mock_draw_png = MagicMock(side_effect=Exception("æç”»ã‚¨ãƒ©ãƒ¼"))
    
    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    with pytest.raises(DashboardError) as exc_info:
        render_dashboard(
            config, metadata, core_data, metrics,
            _draw_png_func=mock_draw_png
        )
    
    assert "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æç”»ã‚¨ãƒ©ãƒ¼" in str(exc_info.value)


def test_render_dashboard_creates_output_dir(
    metadata,
    core_data,
    metrics
):
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè‡ªå‹•ä½œæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
    # å­˜åœ¨ã—ãªã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
    temp_dir = tempfile.mkdtemp(prefix="test_dashboard_parent_")
    nonexistent_dir = Path(temp_dir) / "nonexistent" / "subdir"
    
    try:
        config = EnvironmentConfig(
            jira_domain="https://test.atlassian.net",
            jira_email="test@example.com",
            jira_api_token="token123",
            output_dir=str(nonexistent_dir),
            target_done_rate=0.8,
            axis_mode="percent",
            gemini_api_key="test-api-key",
            gemini_model="gemini-2.5-flash-lite",
            gemini_disable=False,
        )
        
        # ãƒ¢ãƒƒã‚¯ã®draw_pngé–¢æ•°
        mock_draw_png = MagicMock()
        
        # æç”»å®Ÿè¡Œ
        output_path = render_dashboard(
            config, metadata, core_data, metrics,
            _draw_png_func=mock_draw_png
        )
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒä½œæˆã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
        assert nonexistent_dir.exists()
        assert output_path == nonexistent_dir / "sprint_overview.png"
        
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        shutil.rmtree(temp_dir, ignore_errors=True)


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
