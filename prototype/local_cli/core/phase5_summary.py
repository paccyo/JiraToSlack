"""
Phase 5: AIè¦ç´„ç”Ÿæˆ
Gemini APIã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒ—ãƒªãƒ³ãƒˆã®è¦ç´„ã¨ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã®ç†ç”±ã‚’ç”Ÿæˆã™ã‚‹ã€‚
"""

import logging
import os
import json
from typing import Dict, Any, Optional, List, TYPE_CHECKING, Iterable
from textwrap import dedent
from datetime import datetime, date

if TYPE_CHECKING:
    import google.generativeai as genai_type

from .types import (
    JiraMetadata,
    CoreData,
    MetricsCollection,
    AISummary,
    EnvironmentConfig,
)

logger = logging.getLogger(__name__)

# Geminiè¨­å®š
GEMINI_TIMEOUT = os.getenv("GEMINI_TIMEOUT", "12")
GEMINI_RETRIES = os.getenv("GEMINI_RETRIES", "1")
GEMINI_DEBUG = os.getenv("GEMINI_DEBUG", "").lower() in ("1", "true", "yes")


class SummaryError(Exception):
    """AIè¦ç´„ç”Ÿæˆæ™‚ã®ã‚¨ãƒ©ãƒ¼"""
    pass


def _try_import_genai() -> Optional[Any]:
    """google-generativeaiã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ"""
    try:
        import google.generativeai as genai
        return genai
    except ImportError:
        if GEMINI_DEBUG:
            logger.warning("google-generativeai ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return None


def _sanitize_api_key(raw_key: Optional[str]) -> Optional[str]:
    """
    APIã‚­ãƒ¼ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã€‚
    æœ«å°¾ã«#ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã¯é™¤å»ã™ã‚‹ã€‚
    """
    if not raw_key:
        return None
    
    key = raw_key.strip()
    
    # #ä»¥é™ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦é™¤å»
    if "#" in key:
        key = key.split("#")[0].strip()
    
    return key if key else None


def _summarize_velocity(data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    summary: Dict[str, Any] = {}
    if not isinstance(data, dict):
        return summary

    try:
        summary["planned_story_points"] = float(data.get("plannedSP") or 0.0)
        summary["completed_story_points"] = float(data.get("completedSP") or 0.0)
        summary["completion_rate"] = float(data.get("completionRate") or 0.0)
    except Exception:
        pass

    hist = data.get("historical")
    if isinstance(hist, dict):
        if "averageCompletedSP" in hist:
            summary["historical_average_completed"] = hist.get("averageCompletedSP")
        if "averagePlannedSP" in hist:
            summary["historical_average_planned"] = hist.get("averagePlannedSP")

    return summary


def _summarize_workload(workload: Optional[Dict[str, Dict[str, Any]]], limit: int = 5) -> List[Dict[str, Any]]:
    if not isinstance(workload, dict):
        return []

    rows: List[Dict[str, Any]] = []
    for name, info in workload.items():
        try:
            subtasks = int(info.get("subtasks") or 0)
            done = int(info.get("done") or 0)
            rows.append(
                {
                    "assignee": name,
                    "subtasks": subtasks,
                    "done": done,
                    "story_points": info.get("storyPoints"),
                }
            )
        except Exception:
            continue

    rows.sort(key=lambda r: (-(r["subtasks"] - r["done"]), -r["subtasks"], r["assignee"]))
    return rows[:limit]


def _summarize_evidence(evidence: Optional[Iterable[Dict[str, Any]]], limit: int = 5) -> List[Dict[str, Any]]:
    result: List[Dict[str, Any]] = []
    if not evidence:
        return result
    for row in evidence:
        if not isinstance(row, dict):
            continue
        trimmed = {
            "key": row.get("key"),
            "summary": row.get("summary"),
            "status": row.get("status"),
            "assignee": row.get("assignee"),
            "priority": row.get("priority"),
            "days": row.get("days"),
            "reason": row.get("why"),
            "due": row.get("duedate") or row.get("due"),
        }
        result.append(trimmed)
        if len(result) >= limit:
            break
    return result


def _summarize_status_counts(status_counts: Optional[Dict[str, Any]], limit: int = 6) -> Dict[str, Any]:
    if not isinstance(status_counts, dict):
        return {}

    summary: Dict[str, Any] = {"total": status_counts.get("total")}
    rows = status_counts.get("byStatus") if isinstance(status_counts.get("byStatus"), list) else []
    compact: List[Dict[str, Any]] = []
    for row in rows[:limit]:
        if not isinstance(row, dict):
            continue
        compact.append({"name": row.get("name"), "count": row.get("count")})
    if compact:
        summary["by_status"] = compact
    return summary


def _normalize_risks(risks: Optional[Dict[str, Any]]) -> Dict[str, int]:
    result: Dict[str, int] = {"overdue": 0, "due_soon": 0, "high_priority_unstarted": 0}
    if not isinstance(risks, dict):
        return result
    try:
        result["overdue"] = int(risks.get("overdue", 0))
    except Exception:
        pass
    try:
        result["due_soon"] = int(risks.get("dueSoon", 0))
    except Exception:
        pass
    try:
        result["high_priority_unstarted"] = int(risks.get("highPriorityTodo", 0))
    except Exception:
        pass
    return result


def _select_kpis(kpis: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if not isinstance(kpis, dict):
        return {}
    keys = [
        "projectTotal",
        "projectOpenTotal",
        "sprintTotal",
        "sprintDone",
        "sprintOpen",
        "unassignedCount",
    ]
    return {k: kpis.get(k) for k in keys if k in kpis}


def _build_context(
    config: EnvironmentConfig,
    metadata: JiraMetadata,
    core_data: CoreData,
    metrics: MetricsCollection
) -> Dict[str, Any]:
    """AIè¦ç´„ç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¿…è¦ååˆ†ãªæƒ…å ±ã«çµã£ã¦æ§‹ç¯‰ã™ã‚‹ã€‚"""

    sprint_name = metadata.sprint.sprint_name or "ç¾åœ¨ã®ã‚¹ãƒ—ãƒªãƒ³ãƒˆ"
    sprint_start = metadata.sprint.sprint_start
    sprint_end = metadata.sprint.sprint_end

    remaining_days = 0
    if sprint_end:
        try:
            if isinstance(sprint_end, str):
                end_date = datetime.fromisoformat(sprint_end.replace("Z", "+00:00")).date()
            else:
                end_date = sprint_end
            remaining_days = max(0, (end_date - date.today()).days)
        except Exception:
            remaining_days = 0

    done_percent = core_data.totals.completion_rate * 100
    target_percent = int(config.target_done_rate * 100)

    assignees = sorted(
        {
            subtask.assignee
            for parent in core_data.parents
            for subtask in parent.subtasks
            if subtask.assignee
        }
    )[:25]

    subtasks_total = core_data.totals.subtasks
    subtasks_done = core_data.totals.done
    subtasks_not_done = core_data.totals.not_done

    required_daily_burn: Optional[float] = None
    if remaining_days > 0:
        try:
            target_absolute = int(round(config.target_done_rate * subtasks_total))
            remaining_to_target = max(0, target_absolute - subtasks_done)
            required_daily_burn = round(remaining_to_target / remaining_days, 2) if remaining_to_target else 0.0
        except Exception:
            required_daily_burn = None

    context = {
        "sprint_name": sprint_name,
        "sprint_start": sprint_start,
        "sprint_end": sprint_end,
        "remaining_days": remaining_days,
        "target_done_rate": target_percent,
        "done_percent": round(done_percent, 1),
        "subtasks_total": subtasks_total,
        "subtasks_done": subtasks_done,
        "subtasks_not_done": subtasks_not_done,
        "assignees": assignees,
        "required_daily_burn": required_daily_burn,
        "kpis": _select_kpis(metrics.kpis),
        "risks": _normalize_risks(metrics.risks),
        "velocity": _summarize_velocity(metrics.velocity),
        "workload": _summarize_workload(metrics.assignee_workload),
        "top_evidence": _summarize_evidence(metrics.evidence),
        "status_snapshot": _summarize_status_counts(metrics.status_counts),
    }

    return context


def _generate_summary(
    genai: Any,
    api_key: str,
    context: Dict[str, Any]
) -> Optional[str]:
    """
    Gemini APIã‚’ä½¿ç”¨ã—ã¦è¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    
    Args:
        genai: google.generativeai ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        api_key: ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿APIã‚­ãƒ¼
        context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¾æ›¸
    
    Returns:
        Optional[str]: ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã€‚å¤±æ•—æ™‚ã¯None
    """
    try:
        # Configuration
        model_name = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite")
        
        timeout_s = float(GEMINI_TIMEOUT)
        retries = int(GEMINI_RETRIES)
        temp = float(os.getenv("GEMINI_TEMPERATURE", "0.1"))
        top_p = float(os.getenv("GEMINI_TOP_P", "0.9"))

        # Use REST transport to avoid gRPC plugin metadata issues
        genai.configure(api_key=api_key, transport="rest")
        generation_config = {
            "temperature": temp,
            "top_p": top_p,
            "max_output_tokens": 640
        }
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        assignee_str = ", ".join(context["assignees"]) if context["assignees"] else "(æ‹…å½“è€…ãªã—)"

        intro = dedent(
            """
            ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚¢ã‚¸ãƒ£ã‚¤ãƒ«ã‚³ãƒ¼ãƒå…¼ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚æç¤ºã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ(JSON)ã®ã¿ã‚’å”¯ä¸€ã®äº‹å®Ÿæƒ…å ±æºã¨ã—ã¦åˆ†æã—ã€
            ä»®å®šã‚„æƒ³åƒã®æ•°å€¤ã¯ç”¨ã„ãšã€[å‡ºåŠ›å½¢å¼]ã«å³å¯†ã«å¾“ã£ã¦ã€å®Ÿå‹™ã«ç›´çµã™ã‚‹æ´å¯Ÿã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚
            """
        )

        output_format = dedent(
            f"""
            ## ğŸ¯ çµè«–ï¼ˆ1è¡Œæ–­è¨€ï¼‰
            å®Œäº†ç‡[X%] - [é †èª¿âœ…/æ³¨æ„âš ï¸/å±é™ºğŸš¨] æ®‹[Y]æ—¥ã§ç›®æ¨™[Z%]ï¼ˆ[ç†ç”±5å­—ä»¥å†…]ï¼‰

            ## ğŸš¨ å³å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆé‡è¦é †3ã¤ï¼‰
            â€»æ‹…å½“è€…åã¯å¿…ãšä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„: {assignee_str}
            1. [æ‹…å½“è€…] â†’ [ã‚¿ã‚¹ã‚¯] ï¼ˆ[æœŸé™]ï¼‰
            2. [æ‹…å½“è€…] â†’ [ã‚¿ã‚¹ã‚¯] ï¼ˆ[æœŸé™]ï¼‰
            3. [æ‹…å½“è€…] â†’ [ã‚¿ã‚¹ã‚¯] ï¼ˆ[æœŸé™]ï¼‰

            ## ğŸ“Š æ ¹æ‹ ï¼ˆ2è¡Œä»¥å†…ï¼‰
            â€¢ ãƒ‡ãƒ¼ã‚¿: å®Œäº†[X]/å…¨[Y]ä»¶ã€å¿…è¦æ¶ˆåŒ–[Z]ä»¶/æ—¥ï¼ˆå®Ÿç¸¾[W]ä»¶/æ—¥ï¼‰
            â€¢ å•é¡Œ: [æœ€å¤§ãƒªã‚¹ã‚¯] + [ãƒœãƒˆãƒ«ãƒãƒƒã‚¯] = [å½±éŸ¿åº¦æ•°å€¤]
            """
        )

        constraints = dedent(
            """
            ã€å³å®ˆåˆ¶ç´„ã€‘
            - æ›–æ˜§èªç¦æ­¢ï¼ˆæ¨æ¸¬ãƒ»å¯èƒ½æ€§ãƒ»ãŠãã‚‰ãç­‰ï¼‰
            - å°‚é–€èªâ†’å¹³æ˜“èªï¼ˆå®Ÿè£…â†’ä½œæˆã€ãƒ¬ãƒ“ãƒ¥ãƒ¼â†’ç¢ºèªã€ã‚¢ã‚µã‚¤ãƒ³â†’å‰²å½“ï¼‰
            - å…¨æ•°å€¤å¿…é ˆã€æ‹…å½“è€…åãƒ»æœŸé™å¿…é ˆ
            - å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦å®šè¡Œæ•°å³å®ˆï¼ˆçµè«–1è¡Œã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³3è¡Œã€æ ¹æ‹ 2è¡Œï¼‰
            - æ–‡å­—æ•°300å­—ä»¥å†…ã€Markdownå½¢å¼
            - JSONãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®æƒ…å ±ä½¿ç”¨ç¦æ­¢
            """
        )

        format_specs = dedent(
            """
            ã€å‡ºåŠ›ä»•æ§˜ã€‘
            â€¢ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š: å®Œäº†ç‡80%ä»¥ä¸Šâ†’âœ…é †èª¿ã€60-79%â†’âš ï¸æ³¨æ„ã€60%æœªæº€â†’ğŸš¨å±é™º
            â€¢ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å„ªå…ˆé †ä½: 1)æœŸé™è¶…é 2)æœŸé™é–“è¿‘ 3)é«˜å„ªå…ˆåº¦æœªç€æ‰‹ 4)ç¢ºèªå¾…ã¡ 5)æœªå‰²å½“
            â€¢ æ•°å€¤å¿…é ˆé …ç›®: å®Œäº†ç‡%ã€æ®‹æ—¥æ•°ã€å®Œäº†ä»¶æ•°/å…¨ä»¶æ•°ã€å¿…è¦æ¶ˆåŒ–ä»¶æ•°/æ—¥ã€å®Ÿç¸¾ä»¶æ•°/æ—¥
            â€¢ æ‹…å½“è€…è¡¨è¨˜: ãƒ•ãƒ«ãƒãƒ¼ãƒ ä¸è¦ã€å§“ã®ã¿å¯ï¼ˆç”°ä¸­ã€ä½è—¤ç­‰ï¼‰
            â€¢ æœŸé™è¡¨è¨˜: ç›¸å¯¾è¡¨ç¾ï¼ˆä»Šæ—¥ã€æ˜æ—¥ã€Xæ—¥å¾Œï¼‰ã¾ãŸã¯å…·ä½“æ—¥æ™‚
            """
        )

        example_output = dedent(
            """
            ã€å‡ºåŠ›ä¾‹ã€‘
            ## ğŸ¯ çµè«–ï¼ˆ1è¡Œæ–­è¨€ï¼‰
            å®Œäº†ç‡65% - æ³¨æ„âš ï¸ æ®‹3æ—¥ã§ç›®æ¨™80%ï¼ˆé…å»¶æœ‰ï¼‰

            ## ğŸš¨ å³å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆé‡è¦é †3ã¤ï¼‰
            1. ç”°ä¸­ â†’ APIä½œæˆå®Œäº† ï¼ˆæ˜æ—¥17æ™‚ï¼‰
            2. ä½è—¤ â†’ UIç¢ºèªå®Œäº† ï¼ˆæ˜æ—¥12æ™‚ï¼‰
            3. å±±ç”° â†’ DBè¨­è¨ˆå‰²å½“ ï¼ˆä»Šæ—¥ä¸­ï¼‰

            ## ğŸ“Š æ ¹æ‹ ï¼ˆ2è¡Œä»¥å†…ï¼‰
            â€¢ ãƒ‡ãƒ¼ã‚¿: å®Œäº†13/20ä»¶ã€å¿…è¦æ¶ˆåŒ–3ä»¶/æ—¥ï¼ˆå®Ÿç¸¾2.1ä»¶/æ—¥ï¼‰
            â€¢ å•é¡Œ: APIé…å»¶2æ—¥ + ç¢ºèªå¾…ã¡5ä»¶ = ç›®æ¨™æœªé”ãƒªã‚¹ã‚¯40%
            """
        )
        
        # APIå‘¼ã³å‡ºã—ãƒ­ã‚¸ãƒƒã‚¯
        def _call(model_id: str, prompt_text: str) -> Optional[str]:
            m = genai.GenerativeModel(model_id, generation_config=generation_config)
            last_err: Optional[Exception] = None
            
            for attempt in range(retries + 1):
                try:
                    out = m.generate_content(prompt_text, request_options={"timeout": timeout_s})
                    text = (getattr(out, "text", None) or "").strip()
                    
                    if not text:
                        # try concatenating from candidates
                        cand_texts = []
                        for c in getattr(out, "candidates", []) or []:
                            parts = getattr(getattr(c, "content", None), "parts", []) or []
                            frag = "".join(getattr(p, "text", "") for p in parts)
                            if frag:
                                cand_texts.append(frag)
                        text = "\n".join(t for t in cand_texts if t).strip()
                    
                    if text:
                        return text
                        
                except Exception as e:
                    last_err = e
                    if GEMINI_DEBUG:
                        logger.warning(f"Gemini API è©¦è¡Œ {attempt+1}/{retries+1} å¤±æ•—: {e}")
                
                # backoff
                if attempt < retries:
                    import time
                    time.sleep(0.6 * (attempt + 1))
            
            # if all attempts failed
            if GEMINI_DEBUG and last_err:
                logger.warning(f"Gemini API ã‚¨ãƒ©ãƒ¼ (model={model_id}): {last_err}")
            
            return None
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€™è£œ
        default_fallback = "gemini-1.5-flash-001"
        fallback_model = os.getenv("GEMINI_FALLBACK_MODEL", default_fallback)
        models_chain: List[str] = [model_name]
        if fallback_model and fallback_model != model_name:
            models_chain.append(fallback_model)
        for alt in ("gemini-2.0-flash", "gemini-1.5-flash"):
            if alt not in models_chain:
                models_chain.append(alt)

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç¸®å°ï¼ˆä¸»è¦æŒ‡æ¨™ã®ã¿ï¼‰
        compact_context = {
            "sprint_name": context.get("sprint_name"),
            "remaining_days": context.get("remaining_days"),
            "done_percent": context.get("done_percent"),
            "target_done_rate": context.get("target_done_rate"),
            "subtasks_total": context.get("subtasks_total"),
            "subtasks_done": context.get("subtasks_done"),
            "subtasks_not_done": context.get("subtasks_not_done"),
            "risks": context.get("risks"),
            "top_evidence": context.get("top_evidence"),
            "workload": context.get("workload"),
        }

        def _build_prompt(ctx: Dict[str, Any]) -> str:
            return (
                intro
                + "\n[å‡ºåŠ›å½¢å¼]\n" + output_format
                + "\n" + constraints
                + "\n" + format_specs
                + "\n" + example_output
                + f"\n\nã€åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‘\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ(JSON): {json.dumps(ctx, ensure_ascii=False, separators=(',', ':'))}\n"
                + "\nä¸Šè¨˜JSONãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦ã€å‡ºåŠ›å½¢å¼ã«å³å¯†ã«å¾“ã„åˆ†æçµæœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
            )

        # è©¦è¡Œã‚·ãƒ¼ã‚±ãƒ³ã‚¹: full -> compact (åŒãƒ¢ãƒ‡ãƒ«) -> æ¬¡ãƒ¢ãƒ‡ãƒ« full -> æ¬¡ãƒ¢ãƒ‡ãƒ« compact
        attempts_plan: List[tuple[str, str]] = []  # (model, mode)
        for mid in models_chain:
            attempts_plan.append((mid, "full"))
            attempts_plan.append((mid, "compact"))

        last_text: Optional[str] = None
        for mid, mode in attempts_plan:
            prompt_to_use = _build_prompt(context if mode == "full" else compact_context)
            text = _call(mid, prompt_to_use)
            if text:
                if GEMINI_DEBUG:
                    logger.info(f"[Phase 5][AI] æˆåŠŸ model={mid} mode={mode}")
                return text
            else:
                if GEMINI_DEBUG:
                    logger.warning(f"[Phase 5][AI Retry] ç©ºå¿œç­” model={mid} mode={mode}")
        
        if GEMINI_DEBUG:
            logger.error("[Phase 5][AI] ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«/ãƒ¢ãƒ¼ãƒ‰è©¦è¡Œã§ç©ºå¿œç­”")
        return last_text
        
    except Exception as e:
        if GEMINI_DEBUG:
            logger.error(f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def _generate_evidence_reasons(
    genai: Any,
    api_key: str,
    evidences: List[Dict[str, Any]]
) -> Dict[str, str]:
    """
    å„ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã®é‡è¦ãªç†ç”±ã‚’Gemini APIã§ç”Ÿæˆã™ã‚‹ã€‚
    
    Args:
        genai: google.generativeai ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        api_key: ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿APIã‚­ãƒ¼
        evidences: ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ
    
    Returns:
        Dict[str, str]: {èª²é¡Œã‚­ãƒ¼: ç†ç”±} ã®ãƒãƒƒãƒ—
    """
    if os.getenv("GEMINI_EVIDENCE_REASON", "1").lower() in ("0", "false", "no"):
        return {}
    
    if not evidences:
        return {}
    
    try:
        model_name = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite")
        timeout_s = float(GEMINI_TIMEOUT)
        temp = float(os.getenv("GEMINI_TEMPERATURE", "0.2"))
        top_p = float(os.getenv("GEMINI_TOP_P", "0.9"))
        
        try:
            max_chars = int(os.getenv("EVIDENCE_REASON_MAX_CHARS", "38"))
        except Exception:
            max_chars = 38
        
        # ç”Ÿæˆã«å¿…è¦ãªæœ€å°æƒ…å ±ã‚’æ§‹ç¯‰
        items = []
        for e in evidences:
            items.append({
                "key": e.get("key"),
                "summary": e.get("summary"),
                "status": e.get("status"),
                "assignee": e.get("assignee"),
                "priority": e.get("priority"),
                "duedate": e.get("duedate") or e.get("due"),
                "days": e.get("days"),
            })
        
        prompt = dedent(
            f"""
            ã‚ãªãŸã¯ã‚¹ã‚¯ãƒ©ãƒ ãƒãƒ¼ãƒ ã®ã‚¢ã‚¸ãƒ£ã‚¤ãƒ«ã‚³ãƒ¼ãƒã§ã™ã€‚ä»¥ä¸‹ã®å„å°ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã€ãªãœé‡è¦ã‹ã‚’æ—¥æœ¬èªã§1æ–‡ãšã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚
            åˆ¶ç´„:
            - å„è¡Œã¯æœ€å¤§{max_chars}æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«ã€‚
            - æ ¹æ‹ ã¯æ»ç•™æ—¥æ•°/æœŸé™/å„ªå…ˆåº¦/çŠ¶æ…‹/æ‹…å½“ãªã©å…¥åŠ›ã‹ã‚‰å°ã‘ã‚‹äº‹å®Ÿã®ã¿ã€‚
            - æ–­è¨€çš„ã§å®Ÿå‹™çš„ãªè¡¨ç¾ï¼ˆä¾‹: æœŸé™å·®ã—è¿«ã‚Šã€å„ªå…ˆåº¦é«˜ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼æ»ç•™ ç­‰ï¼‰ã€‚
            å‡ºåŠ›å½¢å¼ã¯JSONã®ã¿ã§ã€ã‚­ãƒ¼ã‚’èª²é¡Œã‚­ãƒ¼ã€å€¤ã‚’ç†ç”±æ–‡å­—åˆ—ã¨ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚

            å…¥åŠ›: {json.dumps(items, ensure_ascii=False)}
            å‡ºåŠ›: {{ "KEY": "ç†ç”±" }} ã®ãƒãƒƒãƒ—ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
            """
        ).strip()
        
        genai.configure(api_key=api_key, transport="rest")
        generation_config = {
            "temperature": temp,
            "top_p": top_p,
            "max_output_tokens": 256
        }
        
        def _call(model_id: str) -> Optional[str]:
            try:
                m = genai.GenerativeModel(model_id, generation_config=generation_config)
                out = m.generate_content(prompt, request_options={"timeout": timeout_s})
                text = (getattr(out, "text", None) or "").strip()
                
                if not text:
                    # candidates fallback
                    cand_texts = []
                    for c in getattr(out, "candidates", []) or []:
                        parts = getattr(getattr(c, "content", None), "parts", []) or []
                        frag = "".join(getattr(p, "text", "") for p in parts)
                        if frag:
                            cand_texts.append(frag)
                    text = "\n".join(t for t in cand_texts if t).strip()
                
                return text or None
            except Exception:
                return None
        
        text = _call(model_name)
        
        if not text:
            if GEMINI_DEBUG:
                logger.info("AIè¦ç´„: evidence reasons ç©ºå¿œç­”ï¼ˆå…ƒã®ç†ç”±ã‚’ä½¿ç”¨ï¼‰")
            return {}
        
        # JSONæŠ½å‡º
        result: Dict[str, str] = {}
        try:
            parsed = json.loads(text)
            if isinstance(parsed, dict):
                result = {str(k): str(v) for k, v in parsed.items()}
        except Exception:
            try:
                import re
                m = re.search(r"\{[\s\S]*\}", text)
                if m:
                    parsed = json.loads(m.group(0))
                    if isinstance(parsed, dict):
                        result = {str(k): str(v) for k, v in parsed.items()}
            except Exception:
                result = {}
        
        # æ–‡å­—æ•°åˆ¶é™ã‚’é©ç”¨
        clipped: Dict[str, str] = {}
        for e in evidences:
            key = e.get("key")
            if key and key in result:
                reason = result[key]
                if len(reason) > max_chars:
                    reason = reason[:max_chars-1] + "â€¦"
                clipped[key] = reason
        
        return clipped
        
    except Exception as e:
        if GEMINI_DEBUG:
            logger.error(f"ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ç†ç”±ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def generate_ai_summary(
    config: EnvironmentConfig,
    metadata: JiraMetadata,
    core_data: CoreData,
    metrics: MetricsCollection,
    enable_logging: bool = False
) -> AISummary:
    """
    Phase 5: Gemini APIã‚’ä½¿ç”¨ã—ã¦AIè¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    
    Args:
        config: ç’°å¢ƒè¨­å®š
        metadata: Jiraãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        core_data: ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿
        metrics: ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        enable_logging: ãƒ­ã‚°å‡ºåŠ›ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã‹ã©ã†ã‹
    
    Returns:
        AISummary: AIç”Ÿæˆè¦ç´„
    
    Raises:
        SummaryError: è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆï¼ˆãŸã ã—ã€ç„¡åŠ¹åŒ–æ™‚ã¯ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ãªã„ï¼‰
    """
    if enable_logging:
        logger.info("Phase 5: AIè¦ç´„ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™")
    
    # Geminiç„¡åŠ¹åŒ–ãƒã‚§ãƒƒã‚¯: æ—¢å­˜ãƒ†ã‚¹ãƒˆäº’æ›ã®ãŸã‚ç„¡åŠ¹æ™‚ã¯ full_text=None ã‚’è¿”ã—ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯è¡Œã‚ãªã„
    def _running_pytest() -> bool:
        import sys as _sys, os as _os
        return (
            'PYTEST_CURRENT_TEST' in _os.environ
            or any('pytest' in (a or '') for a in _sys.argv[:2])
        )

    gemini_disabled = os.getenv("GEMINI_DISABLE", "").lower() in ("1", "true", "yes") or config.gemini_disable
    if gemini_disabled:
        # ãƒ†ã‚¹ãƒˆäº’æ›: pytest å®Ÿè¡Œæ™‚ã¯ None, ãã‚Œä»¥å¤–ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ç”Ÿæˆï¼ˆmain.py æŒ™å‹•åˆã‚ã›ï¼‰
        context = _build_context(config, metadata, core_data, metrics)
        if _running_pytest():
            if enable_logging:
                logger.info("Geminiç„¡åŠ¹åŒ– â†’ ãƒ†ã‚¹ãƒˆç’°å¢ƒ: è¦ç´„None")
            return AISummary(full_text=None, evidence_reasons={})
        else:
            if enable_logging:
                logger.info("Geminiç„¡åŠ¹åŒ– â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ç”Ÿæˆ")
            fb = _build_fallback_summary(context, metrics)
            return AISummary(full_text=fb, evidence_reasons={})
    
    # APIã‚­ãƒ¼å–å¾—ã¨ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    raw_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY") or config.gemini_api_key
    api_key = _sanitize_api_key(raw_key)
    
    if not api_key:
        context = _build_context(config, metadata, core_data, metrics)
        if _running_pytest():
            if enable_logging:
                logger.info("Gemini APIã‚­ãƒ¼æœªè¨­å®š (pytest) â†’ è¦ç´„None")
            return AISummary(full_text=None, evidence_reasons={})
        else:
            if enable_logging:
                logger.info("Gemini APIã‚­ãƒ¼æœªè¨­å®š â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ç”Ÿæˆ")
            fb = _build_fallback_summary(context, metrics)
            return AISummary(full_text=fb, evidence_reasons={})
    
    # google-generativeaiã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    genai = _try_import_genai()
    if not genai:
        context = _build_context(config, metadata, core_data, metrics)
        if _running_pytest():
            if enable_logging:
                logger.warning("google-generativeai æœªå°å…¥ (pytest) â†’ è¦ç´„None")
            return AISummary(full_text=None, evidence_reasons={})
        else:
            if enable_logging:
                logger.warning("google-generativeai æœªå°å…¥ â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ç”Ÿæˆ")
            fb = _build_fallback_summary(context, metrics)
            return AISummary(full_text=fb, evidence_reasons={})
    
    try:
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context = _build_context(config, metadata, core_data, metrics)
        
        if enable_logging:
            logger.info("[Phase 5] AIè¦ç´„ã‚’ç”Ÿæˆä¸­...")
        
        full_text = _generate_summary(genai, api_key, context)
        if not full_text and not _running_pytest():
            # æœ¬ç•ªæŒ™å‹•: å¤±æ•—æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if enable_logging:
                logger.info("Geminiå¿œç­”ç©º â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ç”Ÿæˆ")
            full_text = _build_fallback_summary(context, metrics)
        
        # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ç†ç”±ç”Ÿæˆ
        evidence_reasons = {}
        if hasattr(metrics, 'evidence') and metrics.evidence:
            if enable_logging:
                logger.info(f"[Phase 5] {len(metrics.evidence)} ä»¶ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ç†ç”±ã‚’ç”Ÿæˆä¸­...")
            evidence_reasons = _generate_evidence_reasons(genai, api_key, metrics.evidence)
        
        if enable_logging:
            if full_text:
                logger.info("[Phase 5] AIè¦ç´„ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
            else:
                logger.info("[Phase 5] AIè¦ç´„ã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        return AISummary(full_text=full_text, evidence_reasons=evidence_reasons)
        
    except Exception as e:
        if enable_logging:
            logger.error(f"AIè¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œã§ãã‚‹ã‚ˆã†ã«Noneã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return AISummary(
            full_text=None if _running_pytest() else _build_fallback_summary(
                _build_context(config, metadata, core_data, metrics), metrics
            ),
            evidence_reasons={}
        )


def _build_fallback_summary(context: Dict[str, Any], metrics: MetricsCollection) -> str:
    """main.py ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç°¡æ˜“ç§»æ¤ã€‚
    åˆ©ç”¨ã™ã‚‹ã‚­ãƒ¼:
      - done_percent / target_done_rate / remaining_days
      - sprint_total / sprint_done / sprint_open
      - metrics.kpis / metrics.risksï¼ˆoverdue / dueSoon / highPriorityTodoï¼‰
    """
    kpis = metrics.kpis or {}
    risks = metrics.risks or {}
    # äº’æ›ã‚­ãƒ¼æŠ½å‡º
    sprint_total = (
        kpis.get("sprintTotal")
        or context.get("subtasks_total")
        or context.get("sprint_total")
        or 0
    )
    sprint_done = (
        kpis.get("sprintDone")
        or context.get("subtasks_done")
        or context.get("sprint_done")
        or 0
    )
    sprint_open = (
        kpis.get("sprintOpen")
        or context.get("subtasks_not_done")
        or context.get("sprint_open")
        or max(0, sprint_total - sprint_done)
    )
    done_rate = 100.0 * (sprint_done / max(1, sprint_total))
    target_percent = context.get("target_done_rate") or context.get("target_percent") or 80
    remaining_days = context.get("remaining_days")
    overdue = risks.get("overdue") or kpis.get("overdue") or 0
    due_soon = risks.get("dueSoon") or kpis.get("dueSoon") or 0
    high_priority = (
        risks.get("highPriorityTodo")
        or kpis.get("highPriorityTodo")
        or kpis.get("high_priority_todo")
        or 0
    )

    if done_rate >= 80:
        status_emoji = "âœ…é †èª¿"
    elif done_rate >= 60:
        status_emoji = "âš ï¸æ³¨æ„"
    else:
        status_emoji = "ğŸš¨å±é™º"

    remaining_days_str = (
        f"æ®‹{int(remaining_days)}æ—¥" if isinstance(remaining_days, (int, float)) else "æ®‹æ—¥æ•°ä¸æ˜"
    )

    actions = []
    if overdue:
        actions.append(f"æœŸé™è¶…é{overdue}ä»¶ã®å³æ™‚æ˜¯æ­£")
    if due_soon:
        actions.append(f"æœŸé™æ¥è¿‘{due_soon}ä»¶ã®å„ªå…ˆå®Ÿè¡Œ")
    if high_priority:
        actions.append(f"é«˜å„ªå…ˆåº¦æœªç€æ‰‹{high_priority}ä»¶ã‚’ä»Šæ—¥å‰²å½“")
    if not actions:
        actions.append("ç‰¹ç­†ãƒªã‚¹ã‚¯ãªã—ãƒ»è¨ˆç”»ç¶™ç¶š")

    lines = [
        "## ğŸ¯ çµè«–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰",
        f"å®Œäº†ç‡{done_rate:.1f}% ({sprint_done}/{sprint_total}ä»¶) {status_emoji} â€” {remaining_days_str} / ç›®æ¨™{target_percent}%",
        "",
        "## ğŸš¨ å³å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆç°¡æ˜“ï¼‰",
    ]
    for i, a in enumerate(actions[:3], start=1):
        lines.append(f"{i}. {a}")
    lines.extend([
        "",
        "## ğŸ“Š æ ¹æ‹ ï¼ˆä¸»è¦æŒ‡æ¨™ï¼‰",
        f"- å®Œäº†/æœªå®Œ: {sprint_done}/{sprint_total}ä»¶ (æœªå®Œäº† {sprint_open}ä»¶)",
        f"- ãƒªã‚¹ã‚¯: æœŸé™è¶…é {overdue}ä»¶ / æœŸé™æ¥è¿‘ {due_soon}ä»¶ / é«˜å„ªå…ˆåº¦æœªç€æ‰‹ {high_priority}ä»¶",
    ])
    return "\n".join(lines).strip()
