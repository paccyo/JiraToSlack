import importlib
import os
import sys
import json
import stat
import subprocess
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple
from PIL import Image, ImageDraw, ImageFont
import requests
from requests.auth import HTTPBasicAuth
# Ensure the repository root is on sys.path when executed as a script
REPO_ROOT = Path(__file__).resolve().parents[2]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

try:
    from prototype.local_cli.lib.env_loader import ensure_env_loaded, build_process_env
    from prototype.local_cli.lib.board_selector import resolve_board_with_preferences
except ModuleNotFoundError:  # pragma: no cover - fallback when package import fails
    alt_lib_dir = Path(__file__).resolve().parent / "lib"
    if str(alt_lib_dir) not in sys.path:
        sys.path.insert(0, str(alt_lib_dir))
    from env_loader import ensure_env_loaded, build_process_env  # type: ignore
    from board_selector import resolve_board_with_preferences  # type: ignore
try:
    import google.generativeai as genai  # type: ignore
    from google.generativeai import types
except Exception:
    genai = None  # type: ignore
from textwrap import dedent


# Local configuration defaults (managed within code instead of environment variables)
BURNDOWN_UNIT = "issues"
GEMINI_DEBUG = True
GEMINI_TIMEOUT = 40.0
GEMINI_RETRIES = 2
AI_OVERLAY_IN_IMAGE = True
AI_OVERLAY_MAX_LINES = 18


ensure_env_loaded()


def _log_deployment_diagnostics() -> None:
    """Emit detailed diagnostics so Cloud Run / container logs capture environment state."""

    def _safe_print(header: str, lines: Iterable[str]) -> None:
        print(header)
        for line in lines:
            print(line)

    def _list_dir(path: Path, limit: int = 60) -> List[str]:
        rows: List[str] = []
        try:
            entries = sorted(path.iterdir(), key=lambda p: p.name)
        except FileNotFoundError:
            return ["(not found)"]
        except PermissionError:
            return ["(permission denied)"]
        except Exception as exc:  # pragma: no cover - defensive
            return [f"(failed to list: {exc})"]
        for idx, entry in enumerate(entries):
            if idx >= limit:
                rows.append(f"... ({len(entries) - limit} more entries omitted)")
                break
            try:
                info = entry.stat()
                mode = stat.filemode(info.st_mode)
                size = info.st_size
            except Exception:
                mode = "??????????"
                size = 0
            rows.append(f"{mode} {size:>10} {entry.name}")
        if not rows:
            rows.append("(empty directory)")
        return rows

    def _dump_file(path: Path, max_bytes: int = 2000) -> List[str]:
        try:
            data = path.read_text(encoding="utf-8", errors="replace")
        except FileNotFoundError:
            return ["(file not found)"]
        except PermissionError:
            return ["(permission denied)"]
        except Exception as exc:  # pragma: no cover - defensive
            return [f"(failed to read: {exc})"]
        if len(data) > max_bytes:
            snippet = data[:max_bytes] + "\n... (truncated)"
        else:
            snippet = data
        return snippet.splitlines() or ["(file is empty)"]

    def _log_sys_path() -> None:
        print("$ python -c 'import os,sys; ...' (captured via runtime)")
        print(f"cwd = {os.getcwd()}")
        print("--- sys.path ---")
        for entry in sys.path:
            print(entry)

    def _log_import(name: str) -> None:
        print(f"$ python -c 'import {name}' (captured via runtime)")
        try:
            module = importlib.import_module(name)
            location = getattr(module, "__file__", None) or "(namespace package)"
            print(f"import succeeded: {name} -> {location}")
        except Exception as exc:  # pragma: no cover - import diagnostics only
            print(f"import failed: {name} -> {exc}")

    print("=== deployment diagnostics: start ===")

    candidate_roots = [Path(p) for p in ("/workspace", "/app", str(REPO_ROOT))]
    for root in candidate_roots:
        _safe_print(f"$ ls -la {root}", _list_dir(root))
        _safe_print(f"$ ls -la {root / 'prototype'}", _list_dir(root / "prototype"))
        _safe_print(f"$ ls -la {root / 'prototype' / 'local_cli'}", _list_dir(root / "prototype" / "local_cli"))

    target_file = Path("/workspace/prototype/local_cli/__init__.py")
    if not target_file.exists():
        target_file = REPO_ROOT / "prototype" / "local_cli" / "__init__.py"
    _safe_print(f"$ cat {target_file}", _dump_file(target_file))

    _log_sys_path()
    _log_import("prototype")
    _log_import("prototype.local_cli")
    _log_import("prototype.local_cli.lib")

    print("=== deployment diagnostics: end ===")


_log_deployment_diagnostics()

def _sanitize_api_key(raw: Optional[str]) -> Optional[str]:
    if not raw:
        return raw
    s = str(raw).strip().strip('"').strip("'")
    # Prefer substring starting at 'AIza' if present
    start = s.find('AIza')
    if start >= 0:
        s = s[start:]
    # Allowed chars for Google API keys (alnum, '-', '_')
    # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯Jiraãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ã‚¹ãƒ—ãƒªãƒ³ãƒˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚
    # å„é–¢æ•°ãƒ»ä¸»è¦å‡¦ç†ã®å†…éƒ¨ãƒ­ã‚¸ãƒƒã‚¯ã‚„ç›®çš„ã‚’æ—¥æœ¬èªžã‚³ãƒ¡ãƒ³ãƒˆã§è©³ç´°ã«è¨˜è¿°ã—ã¦ã„ã¾ã™ã€‚

    allowed = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_')
    out = []
    for ch in s:
        if ch in allowed:
            out.append(ch)
        else:
            # stop at first invalid char to drop trailing comments like '#API'
            break
    filtered = ''.join(out)
    return filtered or None


def _run_python_script(
    script_path: str,
    args: Optional[List[str]],
    env: Dict[str, str],
) -> Tuple[subprocess.CompletedProcess[str], bool]:
    """Execute a Python script preferring module execution for reliable imports.

    Returns (completed_process, used_module_flag)."""
    script = Path(script_path).resolve()
    repo_root = Path(__file__).resolve().parents[2]
    base_dir = Path(__file__).resolve().parent
    module_cmd: Optional[List[str]] = None
    try:
        rel = script.relative_to(repo_root)
        module_name = ".".join(rel.with_suffix("").parts)
        module_cmd = [sys.executable, "-X", "utf8", "-m", module_name]
        if args:
            module_cmd.extend(args)
    except ValueError:
        module_cmd = None

    direct_cmd = [sys.executable, "-X", "utf8", str(script)]
    if args:
        direct_cmd.extend(args)

    def _exec(cmd: List[str], cwd: Path) -> subprocess.CompletedProcess[str]:
        return subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding="utf-8",
            env=env,
            cwd=str(cwd),
        )

    if module_cmd:
        proc = _exec(module_cmd, repo_root)
        if proc.returncode == 0:
            return proc, True
        print(
            f"[DEBUG] module execution failed (rc={proc.returncode}) for {script_path}, falling back to direct path",
            file=sys.stderr,
        )

    proc = _exec(direct_cmd, base_dir)
    return proc, False


def get_json_from_script(script_path: str, env_extra: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
    import traceback
    import os
    print(f"[DEBUG] get_json_from_script called with: {script_path}")
    print(f"[DEBUG] CWD: {os.getcwd()}")
    print(f"[DEBUG] ENV: JIRA_DOMAIN={os.environ.get('JIRA_DOMAIN')}, JIRA_EMAIL={os.environ.get('JIRA_EMAIL')}, JIRA_API_TOKEN={os.environ.get('JIRA_API_TOKEN')}")
    try:
        env = build_process_env()
        if env_extra:
            # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°ãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚Jiraèªè¨¼ã‚„å„ç¨®è¨­å®šå€¤ã‚’ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦ã‚»ãƒƒãƒˆã€‚
            # ãƒ­ã‚°å‡ºåŠ›ãŒæœ‰åŠ¹ãªå ´åˆã¯èª­ã¿è¾¼ã‚“ã .envãƒ‘ã‚¹ã‚’è¡¨ç¤ºã€‚
            env.update(env_extra)
        env["OUTPUT_JSON"] = "1"
        env["PYTHONUTF8"] = "1"
        proc, used_module = _run_python_script(script_path, None, env)
        print(f"[DEBUG] subprocess returncode={proc.returncode}")
        if used_module:
            print(f"[DEBUG] executed via module import for {script_path}")
        if proc.stdout:
            preview = proc.stdout[-1000:]
            print(f"[DEBUG] subprocess stdout (tail 1000 chars)=\n{preview}")
        if proc.stderr:
            preview_err = proc.stderr[-1000:]
            print(f"[DEBUG] subprocess stderr (tail 1000 chars)=\n{preview_err}", file=sys.stderr)

        if proc.returncode != 0:
            raise RuntimeError(f"ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚³ãƒ¼ãƒ‰ç•°å¸¸: rc={proc.returncode}")

        stripped = proc.stdout.strip().splitlines()
        if not stripped:
            raise RuntimeError("ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹å‡ºåŠ›ãŒç©ºã§ã™")
        line = stripped[-1]
        try:
            return json.loads(line)
        except json.JSONDecodeError as json_err:
            print("[ERROR] JSON decode failed. Full stdout follows:")
            print(proc.stdout)
            raise RuntimeError("ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã®JSONå‡ºåŠ›è§£æžã«å¤±æ•—ã—ã¾ã—ãŸ") from json_err
    except Exception as e:
        print(f"[ERROR] ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"[ERROR] script_path: {script_path}")
        print(f"[ERROR] ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨: {os.path.exists(script_path)}")
        print(f"[ERROR] ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§: {os.listdir(os.path.dirname(script_path)) if os.path.exists(os.path.dirname(script_path)) else 'ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã—'}")
        print(f"[ERROR] Traceback:\n{traceback.format_exc()}")
        raise RuntimeError("ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ") from e


def get_json_from_script_args(script_path: str, args: List[str], env_extra: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
    env = build_process_env()
    # Google APIã‚­ãƒ¼ã®æ­£è¦åŒ–ã€‚ä¸è¦ãªè¨˜å·ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤åŽ»ã—ã€'AIza'ä»¥é™ã®æœ‰åŠ¹éƒ¨åˆ†ã®ã¿æŠ½å‡ºã€‚
    if env_extra:
        env.update(env_extra)
    env["OUTPUT_JSON"] = "1"
    env["PYTHONUTF8"] = "1"
    proc, used_module = _run_python_script(script_path, args, env)
    print(f"[DEBUG] get_json_from_script_args returncode={proc.returncode} path={script_path} args={args}")
    if used_module:
        print(f"[DEBUG] executed via module import for {script_path}")
    if proc.stdout:
        print(f"[DEBUG] subprocess stdout (tail 1000 chars)=\n{proc.stdout[-1000:]}")
    if proc.stderr:
        print(f"[DEBUG] subprocess stderr (tail 1000 chars)=\n{proc.stderr[-1000:]}", file=sys.stderr)
    if proc.returncode != 0:
        raise RuntimeError(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {script_path} {' '.join(args)} (rc={proc.returncode})")
    stripped = proc.stdout.strip().splitlines()
    if not stripped:
        raise RuntimeError(f"ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹å‡ºåŠ›ãŒç©ºã§ã™: {script_path} {' '.join(args)}")
    line = stripped[-1]
    try:
        return json.loads(line)
    except json.JSONDecodeError as json_err:
        print("[ERROR] JSON decode failed. Full stdout follows:")
        print(proc.stdout)
        raise RuntimeError(f"ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹JSONè§£æžã«å¤±æ•—ã—ã¾ã—ãŸ: {script_path} {' '.join(args)}") from json_err

    # æŒ‡å®šPythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã—ã€JSONå‡ºåŠ›ã‚’å–å¾—ã€‚
    # env_extraã§è¿½åŠ ç’°å¢ƒå¤‰æ•°ã‚’æ¸¡ã›ã‚‹ã€‚å¤±æ•—æ™‚ã¯ä¾‹å¤–ã€‚

def api_get(url: str, auth: HTTPBasicAuth, params: Optional[Dict[str, Any]] = None) -> Tuple[int, Optional[Dict[str, Any]], str]:
    try:
        resp = requests.get(
            url,
            auth=auth,
            headers={"Accept": "application/json"},
            params=params,
            timeout=30,
        )
    except requests.RequestException as e:
        return 0, None, f"HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}"

    if resp.status_code == 200:
        try:
            return 200, resp.json(), ""
        except json.JSONDecodeError:
            return 200, None, "ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®JSONè§£æžã«å¤±æ•—ã—ã¾ã—ãŸ"
    else:
        return resp.status_code, None, resp.text


    # å¼•æ•°ä»˜ãPythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã—ã€JSONå‡ºåŠ›ã‚’å–å¾—ã€‚
    # env_extraã§è¿½åŠ ç’°å¢ƒå¤‰æ•°ã‚’æ¸¡ã›ã‚‹ã€‚å¤±æ•—æ™‚ã¯ä¾‹å¤–ã€‚
def _format_search_error(data: Optional[Dict[str, Any]], err: str) -> str:
    if isinstance(data, dict):
        messages = data.get("errorMessages") or data.get("errors")
        if isinstance(messages, list) and messages:
            return " ".join(str(m) for m in messages if m)
        if isinstance(messages, dict) and messages:
            try:
                return json.dumps(messages, ensure_ascii=False)
            except Exception:
                return str(messages)
    return err


def search_jql_page(
    JIRA_DOMAIN: str,
    auth: HTTPBasicAuth,
    jql: str,
    fields: Optional[List[str]],
    max_results: int,
    page_token: Optional[str] = None,
) -> Tuple[int, Optional[Dict[str, Any]], str]:
    params: Dict[str, Any] = {
        "jql": jql,
        "maxResults": max(1, min(max_results, 5000)),
    }
    if fields:
        params["fields"] = ",".join(fields)
    if page_token:
        params["pageToken"] = page_token
    code, data, err = api_get(f"{JIRA_DOMAIN}/rest/api/3/search/jql", auth, params=params)
    if code != 200 or not isinstance(data, dict):
        return code, None, _format_search_error(data, err)
    return code, data, ""


def collect_issue_count(
    JIRA_DOMAIN: str,
    auth: HTTPBasicAuth,
    jql: str,
    batch: int = 500,
) -> Tuple[int, Optional[int], str]:
    total = 0
    page_token: Optional[str] = None
    seen_tokens: set[str] = set()
    while True:
        code, data, err = search_jql_page(JIRA_DOMAIN, auth, jql, ["id"], batch, page_token)
        if code != 200 or not data:
            return code, None, err
        issues = data.get("issues") or []
        total += len(issues)
        page_token = data.get("nextPageToken")
        is_last = data.get("isLast", True)
        if not issues or not page_token or page_token in seen_tokens or is_last:
            break
        seen_tokens.add(page_token)
    return 200, total, ""


def search_issue_keys(JIRA_DOMAIN: str, auth: HTTPBasicAuth, jql: str, limit: int = 10) -> List[str]:
    keys: List[str] = []
    page_token: Optional[str] = None
    seen_tokens: set[str] = set()
    while len(keys) < limit:
        remaining = max(1, limit - len(keys))
        code, data, _ = search_jql_page(JIRA_DOMAIN, auth, jql, ["key"], remaining, page_token)
        if code != 200 or not data:
            break
        issues = data.get("issues") or []
        for it in issues:
            if "key" in it:
                keys.append(str(it.get("key")))
                if len(keys) >= limit:
                    break
        page_token = data.get("nextPageToken")
        is_last = data.get("isLast", True)
        if not page_token or page_token in seen_tokens or is_last or not issues:
            break
        seen_tokens.add(page_token)
    return keys


def resolve_board(JIRA_DOMAIN: str, auth: HTTPBasicAuth) -> Tuple[int, Optional[Dict[str, Any]], str]:
    domain = (JIRA_DOMAIN or "").rstrip("/")
    if not domain:
        return 400, None, "JIRA_DOMAIN ãŒæœªè¨­å®šã§ã™"

    board_id = os.getenv("JIRA_BOARD_ID")
    project_key = os.getenv("JIRA_PROJECT_KEY")

    def fetch(url: str, params: Optional[Dict[str, Any]] = None) -> Tuple[int, Optional[Dict[str, Any]], str]:
        return api_get(url, auth, params=params)

    return resolve_board_with_preferences(domain, fetch, project_key, board_id)


def try_infer_project_key_from_board(JIRA_DOMAIN: str, auth: HTTPBasicAuth, board: Dict[str, Any]) -> Optional[str]:
    loc = (board or {}).get("location") or {}
    pkey = loc.get("projectKey")
    if pkey:
        return str(pkey)
    bid = board.get("id")
    try:
        bid_int = int(bid)
    except Exception:
        return None
    code, detail, _ = api_get(f"{JIRA_DOMAIN}/rest/agile/1.0/board/{bid_int}", auth)
    if code == 200 and detail:
        loc = (detail or {}).get("location") or {}
        pkey = loc.get("projectKey")
        if pkey:
            return str(pkey)
    return None


def count_boards_for_project(JIRA_DOMAIN: str, auth: HTTPBasicAuth, project_key: Optional[str]) -> int:
    params: Dict[str, Any] = {"maxResults": 50}
    if project_key:
        params["projectKeyOrId"] = project_key
    code, data, _ = api_get(f"{JIRA_DOMAIN}/rest/agile/1.0/board", auth, params=params)
    if code == 200 and data:
        return int(len(data.get("values", []) or []))
    return 1


def count_active_sprints_for_board(JIRA_DOMAIN: str, auth: HTTPBasicAuth, board_id: int) -> int:
    code, data, _ = api_get(
        f"{JIRA_DOMAIN}/rest/agile/1.0/board/{board_id}/sprint",
        auth,
        params={"state": "active", "maxResults": 50},
    )
    if code == 200 and data:
        n = int(len(data.get("values", []) or []))
        return n if n > 0 else 1
    return 1

    # ãƒœãƒ¼ãƒ‰æƒ…å ±ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã‚’æŽ¨å®šã€‚location.projectKeyå„ªå…ˆã€ãªã‘ã‚Œã°APIã§è©³ç´°å–å¾—ã€‚

def resolve_active_sprint(JIRA_DOMAIN: str, auth: HTTPBasicAuth, board_id: int) -> Optional[Dict[str, Any]]:
    code, data, _ = api_get(
        f"{JIRA_DOMAIN}/rest/agile/1.0/board/{board_id}/sprint",
        auth,
        params={"state": "active", "maxResults": 50},
    )
    if code == 200 and data:
        vals = data.get("values", []) or []
        if vals:
            sid = vals[0].get("id")
            scode, sdata, _ = api_get(f"{JIRA_DOMAIN}/rest/agile/1.0/sprint/{sid}", auth)
            if scode == 200 and sdata:
                return sdata
            return vals[0]
    return None


    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ç´ã¥ããƒœãƒ¼ãƒ‰æ•°ã‚’å–å¾—ã€‚
def approximate_count(JIRA_DOMAIN: str, auth: HTTPBasicAuth, jql: str) -> Tuple[int, Optional[int], str]:
    return collect_issue_count(JIRA_DOMAIN, auth, jql, batch=500)


def search_count(JIRA_DOMAIN: str, auth: HTTPBasicAuth, jql: str) -> Tuple[int, Optional[int], str]:
    return collect_issue_count(JIRA_DOMAIN, auth, jql, batch=500)


    # JQLã§è¿‘ä¼¼ä»¶æ•°ã‚’å–å¾—ï¼ˆé«˜é€Ÿï¼‰ã€‚
def agile_sprint_count(JIRA_DOMAIN: str, auth: HTTPBasicAuth, sprint_id: int, jql_filter: Optional[str] = None) -> Tuple[int, Optional[int], str]:
    """Count issues in a sprint via Agile API without fetching items.
    Uses maxResults=0 for efficiency. Optional JQL filter applies on top.
    """
    params: Dict[str, Any] = {"maxResults": 0}
    if jql_filter:
        params["jql"] = jql_filter
    try:
        resp = requests.get(
            f"{JIRA_DOMAIN}/rest/agile/1.0/sprint/{int(sprint_id)}/issue",
            auth=auth,
    # JQLã§æ­£ç¢ºãªä»¶æ•°ã‚’å–å¾—ï¼ˆmaxResults=0ã§é«˜é€Ÿï¼‰ã€‚
            headers={"Accept": "application/json"},
            params=params,
            timeout=30,
        )
    except requests.RequestException as e:
        return 0, None, f"HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}"
    if resp.status_code != 200:
        return resp.status_code, None, resp.text
    try:
        data = resp.json()
        total = int(data.get("total", 0))
        return 200, total, ""
    except Exception as e:
        return 200, None, f"JSONè§£æžå¤±æ•—: {e}"


def try_load_font(size: int) -> ImageFont.ImageFont:
    candidates: List[str] = []
    if os.name == "nt":
        candidates = [
            r"C:\\Windows\\Fonts\\meiryo.ttc",       # Meiryo (æ—¥æœ¬èªž)
            r"C:\\Windows\\Fonts\\YuGothR.ttc",      # Yu Gothic Regular
            r"C:\\Windows\\Fonts\\YuGothM.ttc",      # Yu Gothic Medium
    # Agile APIã§ã‚¹ãƒ—ãƒªãƒ³ãƒˆå†…ä»¶æ•°ã‚’å–å¾—ã€‚JQLãƒ•ã‚£ãƒ«ã‚¿ã‚‚æŒ‡å®šå¯ã€‚
            r"C:\\Windows\\Fonts\\msgothic.ttc",     # MS Gothic
            r"C:\\Windows\\Fonts\\msmincho.ttc",     # MS Mincho
            r"C:\\Windows\\Fonts\\segoeui.ttf",      # Fallback (è‹±æ•°å­—)
        ]
    else:
        candidates = [
            "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒŽè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
            "/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒŽè§’ã‚´ ProN W3.otf",
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/noto-cjk/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSansJP-Regular.otf",
        ]
    # Generic fallbacks
    candidates += ["NotoSansCJKjp-Regular.otf", "NotoSansCJKJP-Regular.otf", "NotoSansJP-Regular.otf", "DejaVuSans.ttf", "arial.ttf"]

    for path in candidates:
        try:
            return ImageFont.truetype(path, size)
        except Exception:
            continue
    return ImageFont.load_default()


def fmt_date(dt_str: Optional[str]) -> Optional[str]:
    # OSã”ã¨ã«æ—¥æœ¬èªžãƒ•ã‚©ãƒ³ãƒˆã‚’å„ªå…ˆã—ã¦ãƒ­ãƒ¼ãƒ‰ã€‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€‚
    if not dt_str:
        return None
    try:
        from datetime import datetime
        for fmt in ("%Y-%m-%d", "%Y-%m-%dT%H:%M:%S.%f%z", "%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%S"):
            try:
                d = datetime.strptime(dt_str, fmt)
                return d.strftime("%Y/%m/%d")
            except Exception:
                continue
    except Exception:
        pass
    return dt_str.replace("-", "/")


def maybe_gemini_summary(api_key: Optional[str], context: Dict[str, Any]) -> Optional[str]:
    # Allow forced disable to avoid network calls
    if os.getenv("GEMINI_DISABLE", "").lower() in ("1", "true", "yes"):
        return None
    if not api_key:
        return None
    if not genai:
        if GEMINI_DEBUG:
            print("[Gemini] google-generativeai not installed or failed to import")
        return None
    try:
        # Configuration
        model_name = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite")
        
        timeout_s = float(GEMINI_TIMEOUT)
        retries = int(GEMINI_RETRIES)
        temp = float(os.getenv("GEMINI_TEMPERATURE", "0.1"))
        top_p = float(os.getenv("GEMINI_TOP_P", "0.9"))

        # Use REST transport to avoid gRPC plugin metadata issues
        genai.configure(api_key=api_key, transport="rest")
        generation_config = {"temperature": temp, "top_p": top_p, "max_output_tokens": 640}
        def _call(model_id: str) -> Optional[str]:
            m = genai.GenerativeModel(model_id, generation_config=generation_config)
            last_err: Optional[Exception] = None
            for attempt in range(retries + 1):
                try:
                    out = m.generate_content(prompt, request_options={"timeout": timeout_s})
                    text = (getattr(out, "text", None) or "").strip()
                    if not text:
                        # try concatenating from candidates
                        cand_texts = []
                        for c in getattr(out, "candidates", []) or []:
                            parts = getattr(getattr(c, "content", None), "parts", []) or []
                            frag = "".join(getattr(p, "text", "") for p in parts)
                            if frag:
                                cand_texts.append(frag)
                        text = "\n".join(t for t in cand_texts if t).strip()
                    if text:
                        return text
                except Exception as e:
                    last_err = e
                    if GEMINI_DEBUG:
                        print(f"[Gemini] attempt {attempt+1}/{retries+1} failed: {e}")
                # backoff
                try:
                    import time as _t
                    _t.sleep(0.6 * (attempt + 1))
                except Exception:
                    pass
            # if all attempts failed
            if GEMINI_DEBUG and last_err:
                print(f"[Gemini] error on model {model_id}: {last_err}")
            return None
        # --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå‡ºåŠ›å½¢å¼ã‚’æ•´å½¢ï¼‰ ---
        intro = dedent(
            """
            ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚¢ã‚¸ãƒ£ã‚¤ãƒ«ã‚³ãƒ¼ãƒå…¼ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚æç¤ºã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ(JSON)ã®ã¿ã‚’å”¯ä¸€ã®äº‹å®Ÿæƒ…å ±æºã¨ã—ã¦åˆ†æžã—ã€
            ä»®å®šã‚„æƒ³åƒã®æ•°å€¤ã¯ç”¨ã„ãšã€[å‡ºåŠ›å½¢å¼]ã«åŽ³å¯†ã«å¾“ã£ã¦ã€å®Ÿå‹™ã«ç›´çµã™ã‚‹æ´žå¯Ÿã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚
            """
        )
        
        # æ‹…å½“è€…åãƒªã‚¹ãƒˆã‚’contextã‹ã‚‰æŠ½å‡ºï¼ˆä¾‹: context["assignees"]ï¼‰
        assignees = context.get("assignees")
        if not assignees:
            # ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚„è¦ªã‚¿ã‚¹ã‚¯ã‹ã‚‰æ‹…å½“è€…åã‚’æŠ½å‡º
            names = set()
            for parent in context.get("parents", []):
                if parent.get("assignee"):
                    names.add(parent["assignee"])
                for sub in parent.get("subtasks", []):
                    if sub.get("assignee"):
                        names.add(sub["assignee"])
            assignees = sorted(names)
            context["assignees"] = assignees
        assignee_str = ", ".join(assignees) if assignees else "(æ‹…å½“è€…ãªã—)"
        output_format = dedent(
            f"""
            ## ðŸŽ¯ çµè«–ï¼ˆ1è¡Œæ–­è¨€ï¼‰
            å®Œäº†çŽ‡[X%] - [é †èª¿âœ…/æ³¨æ„âš ï¸/å±é™ºðŸš¨] æ®‹[Y]æ—¥ã§ç›®æ¨™[Z%]ï¼ˆ[ç†ç”±5å­—ä»¥å†…]ï¼‰
            
            ## ðŸš¨ å³å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆé‡è¦é †3ã¤ï¼‰
            â€»æ‹…å½“è€…åã¯å¿…ãšä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‹ã‚‰é¸æŠžã—ã¦ãã ã•ã„: {assignee_str}
            1. [æ‹…å½“è€…] â†’ [ã‚¿ã‚¹ã‚¯] ï¼ˆ[æœŸé™]ï¼‰
            2. [æ‹…å½“è€…] â†’ [ã‚¿ã‚¹ã‚¯] ï¼ˆ[æœŸé™]ï¼‰ 
            3. [æ‹…å½“è€…] â†’ [ã‚¿ã‚¹ã‚¯] ï¼ˆ[æœŸé™]ï¼‰
            
            ## ðŸ“Š æ ¹æ‹ ï¼ˆ2è¡Œä»¥å†…ï¼‰
            â€¢ ãƒ‡ãƒ¼ã‚¿: å®Œäº†[X]/å…¨[Y]ä»¶ã€å¿…è¦æ¶ˆåŒ–[Z]ä»¶/æ—¥ï¼ˆå®Ÿç¸¾[W]ä»¶/æ—¥ï¼‰
            â€¢ å•é¡Œ: [æœ€å¤§ãƒªã‚¹ã‚¯] + [ãƒœãƒˆãƒ«ãƒãƒƒã‚¯] = [å½±éŸ¿åº¦æ•°å€¤]
            """
        )
        
        constraints = dedent(
            """
            ã€åŽ³å®ˆåˆ¶ç´„ã€‘
            - æ›–æ˜§èªžç¦æ­¢ï¼ˆæŽ¨æ¸¬ãƒ»å¯èƒ½æ€§ãƒ»ãŠãã‚‰ãç­‰ï¼‰
            - å°‚é–€èªžâ†’å¹³æ˜“èªžï¼ˆå®Ÿè£…â†’ä½œæˆã€ãƒ¬ãƒ“ãƒ¥ãƒ¼â†’ç¢ºèªã€ã‚¢ã‚µã‚¤ãƒ³â†’å‰²å½“ï¼‰
            - å…¨æ•°å€¤å¿…é ˆã€æ‹…å½“è€…åãƒ»æœŸé™å¿…é ˆ
            - å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦å®šè¡Œæ•°åŽ³å®ˆï¼ˆçµè«–1è¡Œã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³3è¡Œã€æ ¹æ‹ 2è¡Œï¼‰
            - æ–‡å­—æ•°300å­—ä»¥å†…ã€Markdownå½¢å¼
            - JSONãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®æƒ…å ±ä½¿ç”¨ç¦æ­¢
            """
        )
        
        format_specs = dedent(
            """
            ã€å‡ºåŠ›ä»•æ§˜ã€‘
            â€¢ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š: å®Œäº†çŽ‡80%ä»¥ä¸Šâ†’âœ…é †èª¿ã€60-79%â†’âš ï¸æ³¨æ„ã€60%æœªæº€â†’ðŸš¨å±é™º
            â€¢ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å„ªå…ˆé †ä½: 1)æœŸé™è¶…éŽ 2)æœŸé™é–“è¿‘ 3)é«˜å„ªå…ˆåº¦æœªç€æ‰‹ 4)ç¢ºèªå¾…ã¡ 5)æœªå‰²å½“
            â€¢ æ•°å€¤å¿…é ˆé …ç›®: å®Œäº†çŽ‡%ã€æ®‹æ—¥æ•°ã€å®Œäº†ä»¶æ•°/å…¨ä»¶æ•°ã€å¿…è¦æ¶ˆåŒ–ä»¶æ•°/æ—¥ã€å®Ÿç¸¾ä»¶æ•°/æ—¥
            â€¢ æ‹…å½“è€…è¡¨è¨˜: ãƒ•ãƒ«ãƒãƒ¼ãƒ ä¸è¦ã€å§“ã®ã¿å¯ï¼ˆç”°ä¸­ã€ä½è—¤ç­‰ï¼‰
            â€¢ æœŸé™è¡¨è¨˜: ç›¸å¯¾è¡¨ç¾ï¼ˆä»Šæ—¥ã€æ˜Žæ—¥ã€Xæ—¥å¾Œï¼‰ã¾ãŸã¯å…·ä½“æ—¥æ™‚
            """
        )
        
        example_output = dedent(
            """
            ã€å‡ºåŠ›ä¾‹ã€‘
            ## ðŸŽ¯ çµè«–ï¼ˆ1è¡Œæ–­è¨€ï¼‰
            å®Œäº†çŽ‡65% - æ³¨æ„âš ï¸ æ®‹3æ—¥ã§ç›®æ¨™80%ï¼ˆé…å»¶æœ‰ï¼‰

            ## ðŸš¨ å³å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆé‡è¦é †3ã¤ï¼‰
            1. ç”°ä¸­ â†’ APIä½œæˆå®Œäº† ï¼ˆæ˜Žæ—¥17æ™‚ï¼‰
            2. ä½è—¤ â†’ UIç¢ºèªå®Œäº† ï¼ˆæ˜Žæ—¥12æ™‚ï¼‰
            3. å±±ç”° â†’ DBè¨­è¨ˆå‰²å½“ ï¼ˆä»Šæ—¥ä¸­ï¼‰

            ## ðŸ“Š æ ¹æ‹ ï¼ˆ2è¡Œä»¥å†…ï¼‰
            â€¢ ãƒ‡ãƒ¼ã‚¿: å®Œäº†13/20ä»¶ã€å¿…è¦æ¶ˆåŒ–3ä»¶/æ—¥ï¼ˆå®Ÿç¸¾2.1ä»¶/æ—¥ï¼‰
            â€¢ å•é¡Œ: APIé…å»¶2æ—¥ + ç¢ºèªå¾…ã¡5ä»¶ = ç›®æ¨™æœªé”ãƒªã‚¹ã‚¯40%
            """
        )
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
        prompt = (
            intro
            + "\n[å‡ºåŠ›å½¢å¼]\n"
            + output_format
            + "\n" + constraints
            + "\n" + format_specs
            + "\n" + example_output
            + f"\n\nã€åˆ†æžå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‘\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ(JSON): {json.dumps(context, ensure_ascii=False, indent=2)}\n"
            + "\nä¸Šè¨˜JSONãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦ã€å‡ºåŠ›å½¢å¼ã«åŽ³å¯†ã«å¾“ã„åˆ†æžçµæžœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        )
        # Try primary then fallback model
        text = _call(model_name)

        if not text and GEMINI_DEBUG:
            print("[Gemini] empty response from both primary and fallback models")
        return text
    except Exception as e:
        if GEMINI_DEBUG:
            print(f"[Gemini] error: {e}")
        return None


def maybe_gemini_justify_evidences(
    api_key: Optional[str], evidences: List[Dict[str, Any]]
) -> Dict[str, str]:
    """
    å„ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã®ã€Œé‡è¦ãªç†ç”±ã€ã‚’Geminiã§1æ–‡ç”Ÿæˆã—ã€{key: reason} ã‚’è¿”ã™ã€‚
    - ç’°å¢ƒå¤‰æ•° GEMINI_EVIDENCE_REASON=0 ã§ç„¡åŠ¹åŒ–ï¼ˆæ—¢å®š: æœ‰åŠ¹ï¼‰
    - å¤±æ•—æ™‚ã¯ç©ºdictã‚’è¿”ã—ã€å‘¼ã³å‡ºã—å…ƒã§å…ƒã®ç†ç”±ã‚’ç¶­æŒ
    - é•·ã•ä¸Šé™: EVIDENCE_REASON_MAX_CHARSï¼ˆæ—¢å®š 38 æ–‡å­—ã€è¶…éŽæ™‚ã¯çœç•¥ï¼‰
    """
    try:
        if os.getenv("GEMINI_EVIDENCE_REASON", "1").lower() in ("0", "false", "no"):
            return {}
        if os.getenv("GEMINI_DISABLE", "").lower() in ("1", "true", "yes"):
            return {}
        if not api_key or not genai:
            return {}

        model_name = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite")

        timeout_s = float(GEMINI_TIMEOUT)
        temp = float(os.getenv("GEMINI_TEMPERATURE", "0.2"))
        top_p = float(os.getenv("GEMINI_TOP_P", "0.9"))
        try:
            max_chars = int(os.getenv("EVIDENCE_REASON_MAX_CHARS", "38"))
        except Exception:
            max_chars = 38

        # ç”Ÿæˆã«å¿…è¦ãªæœ€å°æƒ…å ±ã‚’æ§‹ç¯‰
        items = []
        for e in evidences:
            items.append({
                "key": e.get("key"),
                "summary": e.get("summary"),
                "status": e.get("status"),
                "assignee": e.get("assignee"),
                "priority": e.get("priority"),
                "duedate": e.get("duedate") or e.get("due"),
                "days": e.get("days"),
            })

        prompt = dedent(
            f"""
            ã‚ãªãŸã¯ã‚¹ã‚¯ãƒ©ãƒ ãƒãƒ¼ãƒ ã®ã‚¢ã‚¸ãƒ£ã‚¤ãƒ«ã‚³ãƒ¼ãƒã§ã™ã€‚ä»¥ä¸‹ã®å„å°ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã€ãªãœé‡è¦ã‹ã‚’æ—¥æœ¬èªžã§1æ–‡ãšã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚
            åˆ¶ç´„:
            - å„è¡Œã¯æœ€å¤§{max_chars}æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«ã€‚
            - æ ¹æ‹ ã¯æ»žç•™æ—¥æ•°/æœŸé™/å„ªå…ˆåº¦/çŠ¶æ…‹/æ‹…å½“ãªã©å…¥åŠ›ã‹ã‚‰å°Žã‘ã‚‹äº‹å®Ÿã®ã¿ã€‚
            - æ–­è¨€çš„ã§å®Ÿå‹™çš„ãªè¡¨ç¾ï¼ˆä¾‹: æœŸé™å·®ã—è¿«ã‚Šã€å„ªå…ˆåº¦é«˜ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼æ»žç•™ ç­‰ï¼‰ã€‚
            å‡ºåŠ›å½¢å¼ã¯JSONã®ã¿ã§ã€ã‚­ãƒ¼ã‚’èª²é¡Œã‚­ãƒ¼ã€å€¤ã‚’ç†ç”±æ–‡å­—åˆ—ã¨ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚

            å…¥åŠ›: {json.dumps(items, ensure_ascii=False)}
            å‡ºåŠ›: {{ "KEY": "ç†ç”±" }} ã®ãƒžãƒƒãƒ—ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
            """
        ).strip()

        genai.configure(api_key=api_key, transport="rest")
        generation_config = {"temperature": temp, "top_p": top_p, "max_output_tokens": 256}

        def _call(model_id: str) -> Optional[str]:
            try:
                m = genai.GenerativeModel(model_id, generation_config=generation_config)
                out = m.generate_content(prompt, request_options={"timeout": timeout_s})
                text = (getattr(out, "text", None) or "").strip()
                if not text:
                    # candidates fallback
                    cand_texts = []
                    for c in getattr(out, "candidates", []) or []:
                        parts = getattr(getattr(c, "content", None), "parts", []) or []
                        frag = "".join(getattr(p, "text", "") for p in parts)
                        if frag:
                            cand_texts.append(frag)
                    text = "\n".join(t for t in cand_texts if t).strip()
                return text or None
            except Exception:
                return None

        text = _call(model_name)  or None
        if not text:
            if GEMINI_DEBUG:
                print("- AIè¦ç´„: evidence reasons ç©ºå¿œç­”ï¼ˆå…ƒã®ç†ç”±ã‚’ä½¿ç”¨ï¼‰")
            return {}

        # JSONæŠ½å‡º
        result: Dict[str, str] = {}
        try:
            parsed = json.loads(text)
            if isinstance(parsed, dict):
                result = {str(k): str(v) for k, v in parsed.items()}
        except Exception:
            try:
                import re as _re
                m = _re.search(r"\{[\s\S]*\}", text)
                if m:
                    parsed = json.loads(m.group(0))
                    if isinstance(parsed, dict):
                        result = {str(k): str(v) for k, v in parsed.items()}
            except Exception:
                result = {}

        # æ–‡å­—æ•°åˆ¶é™ã‚’é©ç”¨
        clipped: Dict[str, str] = {}
        for e in evidences:
            k = e.get("key")
            if not k:
                continue
            v = (result.get(k) or "").strip()
            if v:
                if len(v) > max_chars:
                    # keep room for ellipsis if needed
                    clipped[k] = (v[: max(1, max_chars - 1)] + "â€¦")
                else:
                    clipped[k] = v

        if GEMINI_DEBUG and clipped:
            print(f"- AIè¦ç´„: evidence reasons {len(clipped)}ä»¶ ç”Ÿæˆ")
        return clipped
    except Exception as e:
        if GEMINI_DEBUG:
            print(f"[Gemini] evidence reasons error: {e}")
        return {}

def draw_png(
    output_path: str,
    data: Dict[str, Any],
    boards_n: int,
    sprints_n: int,
    sprint_name: Optional[str],
    sprint_start: Optional[str],
    sprint_end: Optional[str],
    axis_mode: str,
    target_done_rate: float,
    extras: Optional[Dict[str, Any]] = None,
) -> None:
    W, H = 1400, 980
    bg = (255, 255, 255)
    img = Image.new("RGB", (W, H), bg)
    g = ImageDraw.Draw(img)

    # Colorblind-friendly palette
    col_bg = (255, 255, 255)
    col_project = (230, 230, 230)
    col_board_focus = (200, 200, 200)
    col_board_other = (215, 215, 215)
    col_sprint_focus = (210, 210, 210)
    col_sprint_other = (235, 235, 235)
    # Unified palette (traffic-light + neutrals)
    col_task_done = (27, 158, 119)     # green
    col_task_todo = (217, 95, 2)       # orange
    col_outline = (80, 80, 80)
    col_text = (40, 40, 40)
    col_grid = (220, 220, 220)
    col_benchmark = (50, 50, 200)
    col_ok = (32, 158, 84)
    col_warn = (230, 170, 0)
    col_danger = (204, 32, 38)

    padding = 20
    project_bar_h = 40
    board_bar_h = 28
    sprint_bar_h = 22
    gap = 8
    font_xs = try_load_font(11)
    font_sm = try_load_font(12)
    font_md = try_load_font(14)
    font_lg = try_load_font(20)
    font_xl = try_load_font(28)

    def text_wh(text: str, font: ImageFont.ImageFont) -> Tuple[int, int]:
        try:
            bbox = g.textbbox((0, 0), text, font=font)
            return bbox[2] - bbox[0], bbox[3] - bbox[1]
        except Exception:
            return int(g.textlength(text, font=font)), getattr(font, "size", 14)

    def fit_font_for_width(text: str, max_width: int, base_font: ImageFont.ImageFont, min_size: int = 10) -> ImageFont.ImageFont:
        size = getattr(base_font, "size", 14) or 14
        size = int(size)
        while size >= min_size:
            f = try_load_font(size)
            if g.textlength(text, font=f) <= max_width:
                return f
            size -= 1
        return try_load_font(min_size)

    def draw_text_fit(text: str, x: int, y: int, max_width: int, base_font: ImageFont.ImageFont, fill: Tuple[int, int, int]) -> ImageFont.ImageFont:
        f = fit_font_for_width(text, max_width, base_font)
        g.text((x, y), text, font=f, fill=fill)
        return f

    def trim_to_width(text: str, max_width: int, font: ImageFont.ImageFont) -> str:
        if g.textlength(text, font=font) <= max_width:
            return text
        ell = "â€¦"
        if g.textlength(ell, font=font) > max_width:
            return ""
        lo, hi = 0, len(text)
        ans = ""
        while lo <= hi:
            mid = (lo + hi) // 2
            cand = text[:mid] + ell
            if g.textlength(cand, font=font) <= max_width:
                ans = cand
                lo = mid + 1
            else:
                hi = mid - 1
        return ans
    # Project bar (Header left)
    proj_x0, proj_y0 = padding, padding
    header_right_w = int(W * 0.42)
    proj_x1, proj_y1 = W - padding - header_right_w - 12, proj_y0 + project_bar_h

    # (title drawn later after burndown)
    boards_n = max(1, boards_n)
    board_gap = 2
    board_seg_w = (proj_x1 - proj_x0 - (boards_n - 1) * board_gap) // boards_n
    bx = proj_x0
    focus_board_idx = 0
    focus_board_x0 = proj_x0
    focus_board_x1 = proj_x0 + board_seg_w
    # (timestamp drawn later once; avoid duplicate here)
    focus_board_x0 = proj_x0
    for i in range(boards_n):
        fill = col_board_focus if i == focus_board_idx else col_board_other
        g.rectangle([bx, proj_y0 + 6, bx + board_seg_w, proj_y1 - 6], fill=fill, outline=col_outline)
        if i == focus_board_idx:
            focus_board_x0, focus_board_x1 = bx, bx + board_seg_w
        bx += board_seg_w + board_gap

    # Sprints row constrained to the focused board x-range
    spr_y0 = proj_y1 + gap
    spr_y1 = spr_y0 + board_bar_h
    g.rectangle([focus_board_x0, spr_y0, focus_board_x1, spr_y1], fill=col_board_focus, outline=(60, 60, 60))

    sprints_n = max(1, sprints_n)
    sprint_gap = 2
    spr_total_w = focus_board_x1 - focus_board_x0
    # Precompute sprint total from data for ratio calc (avoid undefined total_cnt)
    try:
        _dtot = (data or {}).get("totals", {}) if isinstance(data, dict) else {}
        sprint_total_data = int(_dtot.get("subtasks", 0))
    except Exception:
        sprint_total_data = 0
    # Try to draw sprint width proportional to subtasks share (sprint vs project)
    spr_ratio: Optional[float] = None
    try:
        kpi_data = (extras or {}).get("kpis", {}) if extras else {}
        # current sprint subtasks total (prefer KPI, fallback to data)
        if isinstance(kpi_data, dict):
            sprint_total = int(kpi_data.get("sprintTotal") or sprint_total_data)
        else:
            sprint_total = sprint_total_data
        proj_total = None
        if extras and isinstance(extras.get("project_subtask_count"), dict):
            proj_total = int(extras["project_subtask_count"].get("total") or 0)
        if not proj_total:
            proj_total = int(kpi_data.get("projectTotal") or 0) if isinstance(kpi_data, dict) else 0
        if proj_total and proj_total > 0:
            spr_ratio = max(0.0, min(1.0, float(sprint_total) / float(proj_total)))
    except Exception:
        spr_ratio = None

    sx = focus_board_x0
    focus_s_x0 = sx
    focus_s_x1 = sx + spr_total_w
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¹ãƒ—ãƒªãƒ³ãƒˆãŒ1ä»¶ã®ã¿ â†’ å¸¯ã‚’1æœ¬ï¼ˆActive Sprint Done/Remaining ã®2è‰²ç©ã¿æ£’ï¼‰
    if sprints_n == 1:
        # 1æœ¬ã®å¸¯ã§å®Œäº†/æœªå®Œäº†ã‚’è¡¨ç¤º
        g.rectangle([sx, spr_y0 + 4, sx + spr_total_w, spr_y1 - 4], fill=col_sprint_focus, outline=col_outline)
        focus_s_x0, focus_s_x1 = sx, sx + spr_total_w
        
        # Backlogã‚’åˆ¥ã®å°ã•ãªæ¨ªãƒãƒ¼ã«è¡¨ç¤º
        try:
            kpi_data = (extras or {}).get("kpis", {}) if extras else {}
            project_open_total = int(kpi_data.get("projectOpenTotal", 0))
            sprint_open = int(kpi_data.get("sprintOpen", 0))  # ç›´æŽ¥sprintOpenã‚’ä½¿ç”¨
            backlog_open = max(0, project_open_total - sprint_open)
            
            if backlog_open > 0:
                # Backlogè¡¨ç¤ºç”¨ã®å°ã•ãªãƒãƒ¼ï¼ˆå·¦ä¸Šã‚¹ãƒ—ãƒªãƒ³ãƒˆå¸¯ã®ä¸‹ï¼‰
                backlog_y0 = spr_y1 + 2
                backlog_y1 = backlog_y0 + 12
                backlog_w = min(200, spr_total_w // 3)  # å¹…ã¯åˆ¶é™
                g.rectangle([sx, backlog_y0, sx + backlog_w, backlog_y1], fill=(230, 230, 230), outline=col_outline)
                g.text((sx + 4, backlog_y0 + 1), f"Backlog: {backlog_open}", font=font_xs, fill=col_text)
        except Exception:
            pass
    elif spr_ratio is not None:
        # è¤‡æ•°ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆå°†æ¥æ‹¡å¼µï¼‰ã®å ´åˆã®ã¿å‰²åˆåˆ†å‰²
        cur_w = max(2, int(round(spr_total_w * spr_ratio)))
        other_w = max(0, spr_total_w - cur_w)
        g.rectangle([sx, spr_y0 + 4, sx + cur_w, spr_y1 - 4], fill=col_sprint_focus, outline=col_outline)
        if other_w > 0:
            g.rectangle([sx + cur_w, spr_y0 + 4, sx + cur_w + other_w, spr_y1 - 4], fill=col_sprint_other, outline=col_outline)
        focus_s_x0, focus_s_x1 = sx, sx + cur_w
    else:
        # Fallback to equal segments when ratio is unknown
        spr_seg_w = (spr_total_w - (sprints_n - 1) * sprint_gap) // sprints_n
        focus_s_idx = 0
        focus_s_x0 = sx
        focus_s_x1 = sx + spr_seg_w
        for i in range(sprints_n):
            fill = col_sprint_focus if i == focus_s_idx else col_sprint_other
            g.rectangle([sx, spr_y0 + 4, sx + spr_seg_w, spr_y1 - 4], fill=fill, outline=col_outline)
            if i == focus_s_idx:
                focus_s_x0, focus_s_x1 = sx, sx + spr_seg_w
            sx += spr_seg_w + sprint_gap

    # Tasks row constrained to the focused sprint x-range
    parents = data.get("parents", [])
    tasks: List[Dict[str, Any]] = []
    for p in parents:
        for st in p.get("subtasks", []) or []:
            tasks.append(st)

    task_y0 = spr_y1 + gap
    task_y1 = task_y0 + sprint_bar_h
    g.rectangle([focus_s_x0, task_y0, focus_s_x1, task_y1], fill=col_sprint_focus, outline=(60, 60, 60))

    n = max(1, len(tasks))
    task_gap = 2
    task_total_w = focus_s_x1 - focus_s_x0
    task_seg_w = (task_total_w - (n - 1) * task_gap) // n
    tx = focus_s_x0
    for t in tasks:
        done = bool(t.get("done")) if t.get("done") is not None else False
        fill = col_task_done if done else col_task_todo
        g.rectangle([tx, task_y0 + 3, tx + task_seg_w, task_y1 - 3], fill=fill, outline=None)
        tx += task_seg_w + task_gap

    # Summary bar (Done vs Not Done) with labels â€” use data-based totals (consistency)
    totals = data.get("totals", {})
    done_cnt = int(totals.get("done", 0))
    total_cnt = int(totals.get("subtasks", max(1, len(tasks))))
    not_done_cnt = max(0, total_cnt - done_cnt)
    done_rate = (done_cnt / total_cnt) if total_cnt > 0 else 0.0

    sum_y0 = task_y1 + 14
    sum_y1 = sum_y0 + 26
    g.rectangle([focus_s_x0, sum_y0, focus_s_x1, sum_y1], fill=(245, 245, 245), outline=col_outline)
    done_w = int((focus_s_x1 - focus_s_x0) * done_rate)
    g.rectangle([focus_s_x0, sum_y0, focus_s_x0 + done_w, sum_y1], fill=col_task_done)
    g.rectangle([focus_s_x0 + done_w, sum_y0, focus_s_x1, sum_y1], fill=col_task_todo)
    # Headline above summary bar â€” clarify unit (å°ã‚¿ã‚¹ã‚¯)
    headline = f"ã‚¹ãƒ—ãƒªãƒ³ãƒˆ(å°ã‚¿ã‚¹ã‚¯): {total_cnt}ä»¶ | å®Œäº†: {done_cnt} ({int(done_rate*100)}%)"
    head_x, head_y = focus_s_x0, sum_y0 - 20
    g.text((head_x, head_y), headline, font=font_md, fill=col_text)
    # Compute headline bounding box for collision checks
    try:
        hb = g.textbbox((head_x, head_y), headline, font=font_md)
    except Exception:
        hb = (head_x, head_y, head_x + int(g.textlength(headline, font=font_md)), head_y + getattr(font_md, "size", 14))

    # Numeric labels on segments
    def center_text(x0: int, x1: int, y: int, text: str, font: ImageFont.ImageFont, fill=col_text):
        tw, th = g.textlength(text, font=font), font.size
        cx = (x0 + x1) // 2 - int(tw // 2)
        g.text((cx, y), text, font=font, fill=fill)

    label_y = sum_y0 + 5
    done_label = f"{done_cnt} tasks ({int(done_rate*100)}%)"
    not_label = f"{not_done_cnt} tasks ({int((1-done_rate)*100)}%)"
    if done_w > g.textlength(done_label, font=font_sm) + 8:
        center_text(focus_s_x0, focus_s_x0 + done_w, label_y, done_label, font_sm, fill=(255,255,255))
    if (focus_s_x1 - (focus_s_x0 + done_w)) > g.textlength(not_label, font=font_sm) + 8:
        center_text(focus_s_x0 + done_w, focus_s_x1, label_y, not_label, font_sm, fill=(255,255,255))

    # Axis grid and labels (0,25,50,75,100)
    grid_y0 = sum_y1 + 10
    grid_y1 = grid_y0 + 1
    g.line([focus_s_x0, grid_y0, focus_s_x1, grid_y0], fill=col_outline, width=1)
    ticks = [0, 25, 50, 75, 100]
    for t in ticks:
        x = focus_s_x0 + int((focus_s_x1 - focus_s_x0) * (t / 100.0))
        g.line([x, grid_y0 - 5, x, grid_y0 + 5], fill=col_outline, width=1)
        # light vertical grid lines
        g.line([x, spr_y0, x, sum_y1], fill=col_grid, width=1)
        if axis_mode == "percent":
            g.text((x - 10, grid_y0 + 6), f"{t}%", font=font_sm, fill=col_text)
        else:
            # Convert percent into counts scale
            count_at_t = int(round(total_cnt * (t / 100.0)))
            g.text((x - 10, grid_y0 + 6), str(count_at_t), font=font_sm, fill=col_text)

    # Benchmark line (target done %)
    bx = focus_s_x0 + int((focus_s_x1 - focus_s_x0) * target_done_rate)
    g.line([bx, sum_y0 - 10, bx, sum_y1 + 10], fill=col_benchmark, width=2)
    # Place benchmark label; avoid collision with headline
    tgt_label = f"ç›®æ¨™ {int(target_done_rate*100)}%"
    tgt_pos_top = (bx + 4, sum_y0 - 18)
    try:
        tb = g.textbbox(tgt_pos_top, tgt_label, font=font_sm)
    except Exception:
        tw = int(g.textlength(tgt_label, font=font_sm))
        th = getattr(font_sm, "size", 12)
        tb = (tgt_pos_top[0], tgt_pos_top[1], tgt_pos_top[0] + tw, tgt_pos_top[1] + th)
    # Simple AABB intersect
    def _intersects(a, b):
        ax0, ay0, ax1, ay1 = a
        bx0, by0, bx1, by1 = b
        return not (ax1 < bx0 or bx1 < ax0 or ay1 < by0 or by1 < ay0)
    if _intersects(hb, tb):
        # Move below the bar if colliding
        g.text((bx + 4, sum_y1 + 4), tgt_label, font=font_sm, fill=col_benchmark)
    else:
        g.text(tgt_pos_top, tgt_label, font=font_sm, fill=col_benchmark)

    # (Removed prominent banner to avoid overlap with title area)

    # Header right: Burndown sparkline
    def draw_burndown_sparkline(x0: int, y0: int, w: int, h: int, bd: Optional[Dict[str, Any]]) -> None:
        if not bd:
            g.rectangle([x0, y0, x0 + w, y0 + h], outline=col_outline, fill=(250, 250, 250))
            g.text((x0 + 8, y0 + 8), "ãƒ‡ãƒ¼ã‚¿ãªã—", font=font_sm, fill=(120, 120, 120))
            return
        series = bd.get("timeSeries") or []
        ideal = bd.get("ideal") or []
        if not series:
            g.rectangle([x0, y0, x0 + w, y0 + h], outline=col_outline, fill=(250, 250, 250))
            g.text((x0 + 8, y0 + 8), "ãƒ‡ãƒ¼ã‚¿ãªã—", font=font_sm, fill=(120, 120, 120))
            return
        pad_left = 10
        pad_right = 10
        pad_bottom = 10
        pad_top = 28  # ä¸Šéƒ¨ä½™ç™½ã‚’æ‹¡å¤§ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‹ãƒ©ãƒ™ãƒ«åˆ†ï¼‰
        pad = 10  # äºˆæ¸¬éƒ¨åˆ†ã§ä½¿ç”¨
        gx0, gy0 = x0, y0
        gx1, gy1 = x0 + w, y0 + h
        g.rectangle([gx0, gy0, gx1, gy1], outline=col_outline, fill=(250, 250, 250))
        # axes
        g.line([gx0 + pad_left, gy1 - pad_bottom, gx1 - pad_right, gy1 - pad_bottom], fill=col_outline)
        g.line([gx0 + pad_left, gy0 + pad_top, gx0 + pad_left, gy1 - pad_bottom], fill=col_outline)
        # scale
        # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã¦ã®timeSeriesãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ©ãƒ•ã«åæ˜ 
        filtered_series = series
        rems = [float(x.get("remaining") or 0.0) for x in filtered_series]
        maxv = max(rems) if rems else 1.0
        maxv = max(maxv, 1.0)
        n = len(filtered_series)
        def _get_date(i: int) -> Optional[str]:
            d = filtered_series[i].get("date") or filtered_series[i].get("day") or filtered_series[i].get("time")
            return fmt_date(str(d)) if d else None
        def pt(idx: int, val: float) -> Tuple[int, int]:
            if n <= 1:
                t = 0.0
            else:
                t = idx / (n - 1)
            X = int((gx0 + pad_left) + t * (w - pad_left - pad_right))
            Y = int((gy1 - pad_bottom) - (val / maxv) * (h - pad_top - pad_bottom))
            return X, Y
        # ideal dotted (legend: gray dotted); compute when missing
        if not ideal and n >= 2:
            start_rem = rems[0]
            ideal = [{"remaining": start_rem * (1 - i / (n - 1))} for i in range(n)]
        # build points
        pts_i = [pt(i, float(v.get("remaining") or 0.0)) for i, v in enumerate(ideal[:n])] if ideal else []
        pts = [pt(i, float(v.get("remaining") or 0.0)) for i, v in enumerate(filtered_series)]
        # shade gap between ideal and actual (red when behind => actual above ideal)
        if pts_i and pts and len(pts) == len(pts_i):
            for i in range(1, len(pts)):
                ax0, ay0 = pts[i-1]
                ax1, ay1 = pts[i]
                ix0, iy0 = pts_i[i-1]
                ix1, iy1 = pts_i[i]
                # vertical quads per segment
                poly = [(ax0, ay0), (ax1, ay1), (ix1, iy1), (ix0, iy0)]
                # determine if behind (actual remaining > ideal -> higher Y)
                behind = ((ay0 + ay1) / 2.0) > ((iy0 + iy1) / 2.0)
                shade = (255, 200, 200, 128) if behind else (200, 240, 200, 128)
                try:
                    # use separate overlay for alpha
                    overlay = Image.new("RGBA", (w, h), (0, 0, 0, 0))
                    og = ImageDraw.Draw(overlay)
                    og.polygon([(px - gx0, py - gy0) for (px, py) in poly], fill=shade, outline=None)
                    img.paste(overlay, (gx0, gy0), overlay)
                except Exception:
                    pass
        # draw ideal dotted
        if pts_i:
            for i in range(1, len(pts_i)):
                if i % 2 == 0:
                    g.line([pts_i[i-1], pts_i[i]], fill=(120, 120, 120), width=2)
        # actual solid
        for i in range(1, len(pts)):
            g.line([pts[i-1], pts[i]], fill=(0, 120, 210), width=3)
        # Title and its bbox for collision checks
        title_pos = (gx0 + pad_left, gy0 + 2)  # ä¸Šéƒ¨ä½™ç™½å†…ã«ã‚¿ã‚¤ãƒˆãƒ«ã‚’æç”»
        title_txt = "ãƒãƒ¼ãƒ³ãƒ€ã‚¦ãƒ³ï¼ˆæœªå®Œäº†ã‚¿ã‚¹ã‚¯æŽ¨ç§»ï¼‰"
        g.text(title_pos, title_txt, font=font_md, fill=col_text)
        try:
            title_bb = g.textbbox(title_pos, title_txt, font=font_md)
        except Exception:
            title_bb = (title_pos[0], title_pos[1], title_pos[0] + int(g.textlength(title_txt, font=font_md)), title_pos[1] + getattr(font_md, "size", 14))
        # axis labels (Y ticks and X dates)
        for frac in (0.0, 0.5, 1.0):
            x = gx0 + pad_left
            y = int((gy1 - pad_bottom) - frac * (h - pad_top - pad_bottom))
            g.line([x - 4, y, x + 4, y], fill=col_outline)
            val = int(round(maxv * frac))
            g.text((x - 8 - g.textlength(str(val), font=font_sm), y - 6), str(val), font=font_sm, fill=col_text)
        if n >= 2:
            labels = [0, n // 2, n - 1]
            for idx in labels:
                lx, _ = pt(idx, 0)
                dlab = _get_date(idx)
                if dlab:
                    g.text((lx - 10, gy1 - pad_bottom + 2), dlab, font=font_sm, fill=col_text)
        # latest remaining label
        try:
            if pts:
                last_val = float(filtered_series[-1].get("remaining") or 0.0)
                lx, ly = pts[-1]
                lbl = f"æ®‹: {last_val:.1f}"
                g.text((lx + 6, ly - 10), lbl, font=font_sm, fill=(0, 120, 210))
        except Exception:
            pass
        # forecast: simple linear regression on (i, remaining) -> predict zero
        try:
            import statistics as _stats
            xs = list(range(n))
            ys = rems
            # compute slope and intercept
            x_mean = sum(xs) / n
            y_mean = sum(ys) / n
            denom = sum((x - x_mean) ** 2 for x in xs) or 1.0
            slope = sum((x - x_mean) * (y - y_mean) for x, y in zip(xs, ys)) / denom
            intercept = y_mean - slope * x_mean
            # predict index where remaining ~ 0
            if slope < 0:
                t0 = -intercept / slope
            else:
                t0 = float('inf')
            # draw dashed forecast line from last index to t0
            if t0 != float('inf'):
                # sample M points between last and t0 (clamped)
                start = n - 1
                M = 20
                prev = None
                for j in range(M + 1):
                    t = start + (t0 - start) * (j / M)
                    yv = max(0.0, slope * t + intercept)
                    px, py = pt(int(round(t)), yv)
                    if prev is not None and j % 2 == 0:
                        g.line([prev, (px, py)], fill=(120, 0, 160), width=2)
                    prev = (px, py)
                # label predicted date and delay
                from datetime import datetime, timedelta
                # try to parse last date
                last_date_raw = series[-1].get("date") or series[-1].get("day") or series[-1].get("time")
                if last_date_raw and t0 != float('inf'):
                    try:
                        # assume daily cadence
                        base = datetime.strptime(str(series[0].get("date")), "%Y-%m-%d") if series[0].get("date") else datetime.today()
                        pred_date = base + timedelta(days=int(round(t0)))
                        status = "é…å»¶äºˆæ¸¬" if t0 > (n - 1) else "é–“ã«åˆã†äºˆæ¸¬"
                        pred_txt = f"äºˆæ¸¬å®Œäº†: {pred_date.strftime('%Y/%m/%d')} ({status})"
                        pred_pos = (gx0 + pad + 2, gy0 + 2)
                        # Avoid overlapping title; if intersect, move below title
                        try:
                            pred_bb = g.textbbox(pred_pos, pred_txt, font=font_sm)
                        except Exception:
                            pred_bb = (pred_pos[0], pred_pos[1], pred_pos[0] + int(g.textlength(pred_txt, font=font_sm)), pred_pos[1] + getattr(font_sm, "size", 12))
                        def _inter(a, b):
                            ax0, ay0, ax1, ay1 = a
                            bx0, by0, bx1, by1 = b
                            return not (ax1 < bx0 or bx1 < ax0 or ay1 < by0 or by1 < ay0)
                        if _inter(title_bb, pred_bb):
                            g.text((gx0 + pad + 2, title_bb[3] + 2), pred_txt, font=font_sm, fill=(120, 0, 160))
                        else:
                            g.text(pred_pos, pred_txt, font=font_sm, fill=(120, 0, 160))
                    except Exception:
                        alt_txt = "äºˆæ¸¬å®Œäº†: è¨ˆç®—ä¸å¯"
                        alt_pos = (gx0 + pad + 2, gy0 + 2)
                        try:
                            alt_bb = g.textbbox(alt_pos, alt_txt, font=font_sm)
                        except Exception:
                            alt_bb = (alt_pos[0], alt_pos[1], alt_pos[0] + int(g.textlength(alt_txt, font=font_sm)), alt_pos[1] + getattr(font_sm, "size", 12))
                        if not (alt_bb[1] < title_bb[3] and alt_bb[3] > title_bb[1] and alt_bb[2] > title_bb[0] and alt_bb[0] < title_bb[2]):
                            g.text(alt_pos, alt_txt, font=font_sm, fill=(120, 0, 160))
                        else:
                            g.text((gx0 + pad + 2, title_bb[3] + 2), alt_txt, font=font_sm, fill=(120, 0, 160))
        except Exception:
            pass

    # Position burndown and mini velocity side-by-side in header right
    bd_box_x0 = proj_x1 + 12
    bd_box_y0 = padding
    bd_box_h = 110
    bd_box_w = header_right_w // 2 - 6
    velmini_box_x0 = bd_box_x0 + bd_box_w + 12
    velmini_box_w = header_right_w - bd_box_w - 12
    bd_data = (extras or {}).get("burndown") if extras else None
    draw_burndown_sparkline(bd_box_x0, bd_box_y0, bd_box_w, bd_box_h, bd_data)
    # mini velocity chart (reserved_h is dynamic based on KPI text height)
    def draw_velocity_mini(x0: int, y0: int, w: int, h: int, vel: Optional[Dict[str, Any]], reserved_h: int) -> None:
        # Apply adapter to handle both new and old velocity formats
        vel = adapt_velocity_data(vel)
        if not vel:
            g.rectangle([x0, y0, x0 + w, y0 + h], outline=col_outline, fill=(250, 250, 250))
            g.text((x0 + 8, y0 + 8), "ãƒ‡ãƒ¼ã‚¿ãªã—", font=font_sm, fill=(120, 120, 120))
            return
        pts = vel.get("points") or []
        if not isinstance(pts, list) or len(pts) < 2:
            g.rectangle([x0, y0, x0 + w, y0 + h], outline=col_outline, fill=(250, 250, 250))
            g.text((x0 + 8, y0 + 8), "ãƒ™ãƒ­ã‚·ãƒ†ã‚£ã¯ã‚¹ãƒ—ãƒªãƒ³ãƒˆ2ä»¥é™ã«è¡¨ç¤º", font=font_sm, fill=(120, 120, 120))
            return
        pad = 10
        gx0, gy0 = x0, y0
        gx1, gy1 = x0 + w, y0 + h
        g.rectangle([gx0, gy0, gx1, gy1], outline=col_outline, fill=(250, 250, 250))
        # bars
        n = max(1, len(pts))
        bar_gap = 4
        bar_w = max(4, (w - 2 * pad - (n - 1) * bar_gap) // max(1, n))
        values = [float(p.get("points") or 0.0) for p in pts]
        avg = float(vel.get("avgPoints") or 0.0)
        maxv = max(values + [avg, 1.0])
        for i, v in enumerate(values):
            bx = gx0 + pad + i * (bar_w + bar_gap)
            by = gy1 - pad
            bh = int((v / maxv) * (h - 2 * pad - max(0, reserved_h)))
            g.rectangle([bx, by - bh, bx + bar_w, by], fill=(80, 170, 240), outline=col_outline)
        # avg line
        if maxv > 0:
            y_avg = int((gy1 - pad) - (avg / maxv) * (h - 2 * pad - max(0, reserved_h)))
            g.line([gx0 + pad, y_avg, gx1 - pad, y_avg], fill=(120, 0, 120), width=2)
        # draw a small caption at bottom-left to avoid header overlap
        g.text((gx0 + pad, gy1 - pad - 14), "Velocity", font=font_sm, fill=col_text)

    vel_data_hdr = (extras or {}).get("velocity") if extras else None
    # header metrics â€” emphasize progress vs target, avoid zero by falling back to data totals
    try:
        kpis_hdr = (extras or {}).get("kpis", {}) if extras else {}
        proj_total = int(kpis_hdr.get("projectTotal", 0))
        # fallback to subtask totals for sprint numbers to ensure consistency
        sprint_total_kpi = int(kpis_hdr.get("sprintTotal", 0))
        sprint_done_kpi = int(kpis_hdr.get("sprintDone", 0))
        sprint_total = sprint_total_kpi 
        sprint_done = sprint_done_kpi 
        done_pct = int(round(100 * (sprint_done / max(1, sprint_total))))
        tgt_pct = int(round(100 * target_done_rate))
        tx = velmini_box_x0 + 10
        ty = bd_box_y0 + 6
        label = f"é€²æ— {done_pct}% / ç›®æ¨™ {tgt_pct}%"
        max_text_w = velmini_box_w - 18
        # Pre-fit fonts to compute reserved height for chart
        f1 = fit_font_for_width(label, max_text_w, font_lg)
        f2 = fit_font_for_width(
            f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ:{proj_total} | ã‚¹ãƒ—ãƒªãƒ³ãƒˆ(å°ã‚¿ã‚¹ã‚¯):{sprint_total} å®Œäº†:{sprint_done}",
            max_text_w,
            font_sm,
        )
        h1 = text_wh(label, f1)[1]
        line2 = f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ:{proj_total} | ã‚¹ãƒ—ãƒªãƒ³ãƒˆ(å°ã‚¿ã‚¹ã‚¯):{sprint_total} å®Œäº†:{sprint_done}"
        h2 = text_wh(line2, f2)[1]
        # Dedicated KPI panel above mini-velocity
        reserved_h = h1 + 4 + h2 + 10  # inner paddings
        # Ensure we leave room for the mini velocity chart
        min_vel_h = 40
        if reserved_h > bd_box_h - min_vel_h:
            reserved_h = max(28, bd_box_h - min_vel_h)
        # Draw KPI panel box
        g.rectangle([velmini_box_x0, bd_box_y0, velmini_box_x0 + velmini_box_w, bd_box_y0 + reserved_h], outline=col_outline, fill=(255, 255, 255))
        used_font = draw_text_fit(label, tx, ty, max_text_w, font_lg, (col_danger if done_pct < tgt_pct else col_ok))
        ty2 = ty + text_wh(label, used_font)[1] + 4
        draw_text_fit(line2, tx, ty2, max_text_w, font_sm, col_text)
        # Draw mini velocity below KPI panel
        mv_y0 = bd_box_y0 + reserved_h + 6
        mv_h = max(min_vel_h, bd_box_h - reserved_h - 6)
        draw_velocity_mini(velmini_box_x0, mv_y0, velmini_box_w, mv_h, vel_data_hdr, 0)
    except Exception:
        pass
    # Timestamp will be drawn at footer (moved from header to avoid collisions)

    # Title with sprint name and date range (Japanese formatting)
    title = "ã‚¹ãƒ—ãƒªãƒ³ãƒˆ"
    if sprint_name:
        title = f"ã‚¹ãƒ—ãƒªãƒ³ãƒˆ {sprint_name}"
    d0 = fmt_date(sprint_start) if sprint_start else None
    d1 = fmt_date(sprint_end) if sprint_end else None
    if d0 and d1:
        title = f"{title} ({d0} - {d1})"
    # draw safely within canvas (avoid negative y)
    g.text((proj_x0, max(4, proj_y0 - 2)), title, font=font_lg, fill=col_text)

    # Annotation for high not-done ratio and remember its bottom to avoid overlap with next blocks
    annotation_bottom = sum_y1
    if (1 - done_rate) >= 0.4:
        ann_text = f"æœªå®Œäº†ãŒ{int((1-done_rate)*100)}%ã¨é«˜ã„"
        ann_x = focus_s_x0
        ann_y = sum_y1 + 36
        g.text((ann_x, ann_y), ann_text, font=font_md, fill=col_danger)
        annotation_bottom = ann_y + text_wh(ann_text, font_md)[1]

    # Left column blocks below header: Velocity and Status Distribution
    left_col_x0 = proj_x0
    # Push left column below annotation if needed
    left_col_y0 = max(grid_y0 + 40, annotation_bottom + 8)
    left_col_w = proj_x1 - proj_x0
    
    # Adapter function to handle both old and new velocity data formats
    def adapt_velocity_data(vel: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Convert new velocity format to old format for backward compatibility."""
        if not vel:
            return None
        
        # If already in old format, return as-is
        if "points" in vel and "avgPoints" in vel:
            return vel
        
        # Convert new format to old format
        if "history" in vel and "avg" in vel:
            points = []
            for h in vel.get("history", []):
                points.append({
                    "sprintId": h.get("id"),
                    "sprintName": h.get("name"),
                    "points": h.get("points", 0.0)
                })
            
            return {
                "board": vel.get("board"),
                "fieldId": vel.get("fieldId", "customfield_10016"),  # fallback
                "points": points,
                "avgPoints": vel.get("avg", 0.0)
            }
        
        return vel
    
    # Velocity bars
    def draw_velocity(x0: int, y0: int, w: int, h: int, vel: Optional[Dict[str, Any]]) -> int:
        # Apply adapter to handle both new and old velocity formats
        vel = adapt_velocity_data(vel)
        if not vel:
            return y0
        pts = vel.get("points") or []
        avg = float(vel.get("avgPoints") or 0.0)
        if not isinstance(pts, list):
            return y0
        pad = 10
        gx0, gy0 = x0, y0
        gx1, gy1 = x0 + w, y0 + h
        g.rectangle([gx0, gy0, gx1, gy1], outline=col_outline, fill=(250, 250, 250))
        g.text((gx0 + pad, gy0 + 2), "Velocity (last sprints)", font=font_md, fill=col_text)
        # bars
        n = max(1, len(pts))
        bar_gap = 6
        bar_w = max(6, (w - 2 * pad - (n - 1) * bar_gap) // max(1, n))
        maxv = max([float(p.get("points") or 0.0) for p in pts] + [avg, 1.0])
        for i, p in enumerate(pts):
            v = float(p.get("points") or 0.0)
            bx = gx0 + pad + i * (bar_w + bar_gap)
            by = gy1 - pad
            bh = int((v / maxv) * (h - 2 * pad))
            g.rectangle([bx, by - bh, bx + bar_w, by], fill=(80, 170, 240), outline=col_outline)
        # avg line
        if maxv > 0:
            y_avg = int((gy1 - pad) - (avg / maxv) * (h - 2 * pad))
            g.line([gx0 + pad, y_avg, gx1 - pad, y_avg], fill=(120, 0, 120), width=2)
            g.text((gx0 + pad + 4, y_avg - 14), f"avg {avg:.1f}", font=font_sm, fill=(120, 0, 120))
        # target line (dashed)
        try:
            target = float(os.getenv("VELOCITY_TARGET", ""))
        except Exception:
            target = None  # type: ignore
        if maxv > 0 and target:
            y_t = int((gy1 - pad) - (float(target) / maxv) * (h - 2 * pad))
            # dashed line
            x = gx0 + pad
            while x < gx1 - pad:
                x2 = min(x + 10, gx1 - pad)
                g.line([x, y_t, x2, y_t], fill=(200, 0, 0), width=2)
                x += 16
            g.text((gx0 + pad + 4, y_t + 2), f"target {float(target):.1f}", font=font_sm, fill=(200, 0, 0))
        return gy1

    vel_box_h = 140
    vel_data = (extras or {}).get("velocity") if extras else None
    vel_y1 = draw_velocity(left_col_x0, left_col_y0, left_col_w, vel_box_h, vel_data)

    # Status distribution stacked bar
    def draw_status_dist(x0: int, y0: int, w: int, h: int, st: Optional[Dict[str, Any]]) -> int:
        if not st:
            return y0
        bys = st.get("byStatus") or []
        total = float(st.get("total") or 0.0)
        if total <= 0 or not bys:
            return y0
        g.text((x0, y0 - 18), "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ†å¸ƒ", font=font_md, fill=col_text)
        gx0, gy0 = x0, y0
        gx1, gy1 = x0 + w, y0 + h
        g.rectangle([gx0, gy0, gx1, gy1], outline=col_outline, fill=(250, 250, 250))
        # palette rotate
        palette = [
            (200, 200, 200),
            (253, 174, 97),
            (44, 162, 95),
            (49, 130, 189),
            (141, 160, 203),
            (230, 85, 13),
        ]
        x = gx0 + 10
        y = gy0 + 10
        hbar = h - 20
        for i, row in enumerate(bys):
            cnt = float(row.get("count") or 0.0)
            frac = cnt / total
            wseg = int((w - 20) * frac)
            col = palette[i % len(palette)]
            g.rectangle([x, y, x + wseg, y + hbar], fill=col, outline=col_outline)
            label_full = f"{row.get('status')} ({int(frac*100)}%)"
            # If label doesn't fit, fallback to percentage only
            label = label_full if (wseg >= g.textlength(label_full, font=font_sm) + 8) else f"{int(frac*100)}%"
            g.text((x + 4, y + hbar//2 - 8), label, font=font_sm, fill=(30, 30, 30))
            x += wseg
        return gy1

    st_box_y0 = vel_y1 + 24
    st_box_h = 60
    st_data = (extras or {}).get("status_counts") if extras else None
    st_y1 = draw_status_dist(left_col_x0, st_box_y0, left_col_w, st_box_h, st_data)

    # Time-in-Status heatmap (avg days per status)
    def draw_time_in_status_heatmap(x0: int, y0: int, w: int, h: int, tis: Optional[Dict[str, Any]]) -> int:
        if not tis:
            return y0
        per_issue = tis.get("perIssue") or []
        if not per_issue:
            return y0
        # aggregate average days per status
        sum_map: Dict[str, float] = {}
        cnt_map: Dict[str, int] = {}
        vals_map: Dict[str, List[float]] = {}

        # normalize function to merge same meanings (e.g., IN_PROGRESS vs In Progress)
        def norm_status(name: str) -> str:
            s = str(name or "").strip()
            if not s:
                return s
            key = s.lower().replace(" ", "_")
            # common mappings
            aliases = {
                "in_progress": "In Progress",
                "inprogress": "In Progress",
                "in-progress": "In Progress",
                "todo": "To Do",
                "to_do": "To Do",
                "to-do": "To Do",
                "in_review": "In Review",
                "inreview": "In Review",
                "qa": "QA",
                "quality_assurance": "QA",
                "done": "Done",
                "review": "Review",
            }
            return aliases.get(key, s)
        for row in per_issue:
            by = row.get("byStatus") or {}
            for st, days in by.items():
                try:
                    d = float(days)
                except Exception:
                    d = 0.0
                label = norm_status(st)
                sum_map[label] = sum_map.get(label, 0.0) + d
                cnt_map[label] = cnt_map.get(label, 0) + 1
                vals_map.setdefault(label, []).append(d)
        if not sum_map:
            return y0
        items = [(k, (sum_map[k] / max(1, cnt_map.get(k, 1)))) for k in sum_map.keys()]
        # sort by avg days desc
        items.sort(key=lambda x: -x[1])
        # limit to max statuses for display (default 6)
        try:
            max_statuses = int(os.getenv("TIS_MAX_STATUSES", "6"))
        except Exception:
            max_statuses = 6
        items = items[:max(1, max_statuses)]
        g.text((x0, y0 - 18), "å·¥ç¨‹æ»žåœ¨æ™‚é–“ï¼ˆæ—¥ï¼‰(avg | median)", font=font_md, fill=col_text)
        # layout grid 1 row, N columns (small heatmap)
        pad = 8
        n = len(items)
        if n <= 0:
            return y0
        cell_w = max(60, (w - 2 * pad) // n)
        cell_h = h - 2 * pad - 18
        gx0, gy0 = x0, y0
        gx1, gy1 = x0 + w, y0 + h
        g.rectangle([gx0, gy0, gx1, gy1], outline=col_outline, fill=(250, 250, 250))
        # color scale (green -> yellow -> red)
        max_days = max([v for _, v in items] + [1.0])
        def color_for(value: float) -> Tuple[int, int, int]:
            t = min(1.0, value / max_days)
            # 0 -> green(44,162,95), 1 -> red(215,48,39) via yellow
            # simple linear blend green->red
            g0 = (44, 162, 95)
            r1 = (215, 48, 39)
            r = int(g0[0] + (r1[0] - g0[0]) * t)
            gch = int(g0[1] + (r1[1] - g0[1]) * t)
            b = int(g0[2] + (r1[2] - g0[2]) * t)
            return (r, gch, b)
        x = x0 + pad
        y = y0 + pad + 12
        for name, avgd in items:
            col = color_for(avgd)
            g.rectangle([x, y, x + cell_w - 6, y + cell_h], fill=col, outline=col_outline)
            # median label
            try:
                vals = vals_map.get(name, [])
                med = 0.0
                if vals:
                    svals = sorted(vals)
                    m = len(svals)
                    med = (svals[m//2] if m % 2 == 1 else (svals[m//2 - 1] + svals[m//2]) / 2.0)
                g.text((x + 4, y - 14), f"{avgd:.1f}/{med:.1f}d", font=font_sm, fill=col_text)
            except Exception:
                g.text((x + 4, y - 14), f"{avgd:.1f}d", font=font_sm, fill=col_text)
            # label (status name) under cell ifã‚¹ãƒšãƒ¼ã‚¹
            g.text((x + 4, y + cell_h + 2), name[:12], font=font_sm, fill=col_text)
            x += cell_w
        return gy1

    tis_box_y0 = st_y1 + 24
    tis_box_h = 100
    tis_data = (extras or {}).get("time_in_status") if extras else None
    tis_y1 = draw_time_in_status_heatmap(left_col_x0, tis_box_y0, left_col_w, tis_box_h, tis_data)

    # Right column: KPI cards and Assignee workload
    right_x0 = bd_box_x0
    right_y0 = bd_box_y0 + bd_box_h + 16
    right_w = header_right_w

    def draw_kpi_cards(x0: int, y0: int, w: int, h: int, kpis: Dict[str, int]) -> int:
        pad = 8
        cols = 3
        rows = 2
        gap = 10
        card_w = (w - (cols - 1) * gap)
        card_w = card_w // cols
        card_h = h
        # six KPI cards
        order = [
            ("projectOpenTotal", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…æœªå®Œäº†ã‚¿ã‚¹ã‚¯æ•°", (200, 100, 40)),  # æœªå®Œäº†ã‚¿ã‚¹ã‚¯æ•°ã«å¤‰æ›´
            ("sprintOpen", "ã‚¹ãƒ—ãƒªãƒ³ãƒˆå†…æœªå®Œäº†ã‚¿ã‚¹ã‚¯æ•°", (60, 160, 60)),  # ç·ã‚¿ã‚¹ã‚¯æ•°ã‹ã‚‰æœªå®Œäº†ã‚¿ã‚¹ã‚¯æ•°ã«å¤‰æ›´
            ("unassignedCount", "æ‹…å½“è€…æœªå®šã‚¿ã‚¹ã‚¯æ•°", (27, 158, 119)),  # å®Œäº†ã‚¿ã‚¹ã‚¯æ•°ã‹ã‚‰æ‹…å½“è€…æœªå®šã‚¿ã‚¹ã‚¯æ•°ã«å¤‰æ›´
            ("overdue", "æœŸé™éµå®ˆä¸­âœ…", (60, 140, 60)),
            ("dueSoon", "æ³¨æ„:7æ—¥ä»¥å†…æœŸé™", (230, 140, 0)),
            ("highPriorityTodo", "è¦æ³¨æ„ã‚¿ã‚¹ã‚¯(é«˜å„ªå…ˆåº¦)", (200, 120, 60)),
        ]
        x = x0
        y = y0
        for idx, (key, title, col) in enumerate(order):
            v = int(kpis.get(key, 0))
            g.rectangle([x, y, x + card_w, y + card_h], outline=col_outline, fill=(245, 245, 245))
            g.text((x + 8, y + 6), title, font=font_sm, fill=col_text)
            # Positive phrasing for zero overdue
            if key == "overdue" and v == 0:
                txt = "0"
                col_draw = col_ok
            elif key == "sprintOpen":
                # æœªå®Œäº† / ç·æ•° ã®å½¢å¼ã§è¡¨ç¤º
                sprint_total = int(kpis.get("sprintTotal", 0))
                txt = f"{v}/{sprint_total}"
                col_draw = col
            elif key == "projectOpenTotal":
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…æœªå®Œäº† / ç·æ•° ã®å½¢å¼ã§è¡¨ç¤º
                project_total = int(kpis.get("projectTotal", 0))
                txt = f"{v}/{project_total}"
                col_draw = col
            else:
                txt = str(v)
                col_draw = col
            g.text((x + 8, y + 28), txt, font=try_load_font(24), fill=col_draw)
            # advance grid
            if (idx + 1) % cols == 0:
                x = x0
                y += card_h + gap
            else:
                x += card_w + gap
        return y

    kpi_h = 64
    kpi_data = (extras or {}).get("kpis") if extras else {}
    kpi_y1 = draw_kpi_cards(right_x0, right_y0, right_w, kpi_h, kpi_data or {})

    def draw_workload(x0: int, y0: int, w: int, h: int, wl: Optional[Dict[str, Any]]) -> int:
        if not wl:
            return y0
        rows = wl.get("byAssignee") or []
        if not rows:
            return y0
        g.text((x0, y0 - 18), "æ‹…å½“è€…åˆ¥ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆæœªå®Œäº†ï¼‰", font=font_md, fill=col_text)
        pad = 8
        topn = min(8, len(rows))
        rows = sorted(rows, key=lambda r: -int(r.get("notDone") or 0))[:topn]
        maxv = max([int(r.get("notDone") or 0) for r in rows] + [1])
        bar_h = max(14, (h - 2 * pad - (topn - 1) * 6) // max(1, topn))
        y = y0
        for r in rows:
            name = str(r.get("name"))
            v = int(r.get("notDone") or 0)
            bw = int((w - 2 * pad) * (v / maxv))
            g.rectangle([x0, y, x0 + bw, y + bar_h], fill=(255, 180, 70), outline=col_outline)
            g.text((x0 + 6, y + 2), f"{name} ({v})", font=font_sm, fill=(20, 20, 20))
            y += bar_h + 6
        return y

    wl_h = 220
    wl_data = (extras or {}).get("workload") if extras else None
    wl_y1 = draw_workload(right_x0, kpi_y1 + 20, right_w, wl_h, wl_data)
    wl_y2 = wl_y1 + wl_h

    # Footer: Evidence table
    def draw_evidence(x0: int, y0: int, w: int, h: int, ev: Optional[List[Dict[str, Any]]]) -> None:
        g.text((x0, y0 - 18), "é‡è¦ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ï¼ˆTopï¼‰", font=font_md, fill=col_text)
        if not ev:
            return
        # header
        # èª²é¡Œåˆ—ã«ã‚µãƒžãƒªãƒ¼ã‚‚ä½µè¨˜ã™ã‚‹ãŸã‚å¹…ã‚’åºƒã’ã‚‹
        # èª²é¡Œ/æ‹…å½“è€…/ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹/ç†ç”±/ãƒªãƒ³ã‚¯ã®æ¯”çŽ‡ã‚’èª¿æ•´ã—ã¦ãƒªãƒ³ã‚¯åˆ—ã®é–‹å§‹ä½ç½®ã‚’å·¦ã¸å¯„ã›ã‚‹
        # ãƒªãƒ³ã‚¯ã¯çŸ­ç¸®è¡¨ç¤ºã™ã‚‹ãŒã€é–‹å§‹ä½ç½®ãŒå·¦ã«æ¥ã‚‹ã‚ˆã†æœ€å¾Œã®2åˆ—ã‚’ç¸®ã‚ã‚‹
        # åˆ—å¹…: ãƒªãƒ³ã‚¯ã‚’æœ€å·¦ãƒ»æœ€å¤§å¹…ã€ä»–ã¯æœ€å°é™
        col_w = [int(w*0.40), int(w*0.18), int(w*0.12), int(w*0.18), int(w*0.12)]
        headers = ["ãƒªãƒ³ã‚¯", "èª²é¡Œ", "æ‹…å½“è€…", "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", "é‡è¦ãªç†ç”±"]
        cx = x0
        y = y0
        g.rectangle([x0, y0, x0 + w, y0 + h], outline=col_outline, fill=(250, 250, 250))
        # header row
        y_row = y + 6
        cx = x0 + 8
        for i, head in enumerate(headers):
            g.text((cx, y_row), head, font=font_sm, fill=col_text)
            cx += col_w[i]
        y_row += 20
        # data rows
        for e in ev:
            cx = x0 + 8
            # ãƒªãƒ³ã‚¯ï¼ˆå…¨è¡¨ç¤ºã€ã¯ã¿å‡ºã—æ™‚ã¯æœ«å°¾...ï¼‰
            raw_link = str(e.get('link', '')).replace('https://', '').replace('http://', '')
            link = raw_link
            while g.textlength(link, font=font_sm) > (col_w[0] - 12) and len(link) > 4:
                link = link[:-1]
            if g.textlength(link, font=font_sm) > (col_w[0] - 12):
                link = link[:max(0, len(link)-3)] + '...'
            g.text((cx, y_row), link, font=font_sm, fill=col_text)
            cx += col_w[0]
            # èª²é¡Œ: key + summary (truncated)
            key_sum = f"{e.get('key', '')} {e.get('summary', '')[:20]}".strip()
            g.text((cx, y_row), key_sum, font=font_sm, fill=col_text)
            cx += col_w[1]
            # æ‹…å½“è€…
            assignee = e.get('assignee', '')[:10]
            g.text((cx, y_row), assignee, font=font_sm, fill=col_text)
            cx += col_w[2]
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            status = e.get('status', '')[:10]
            g.text((cx, y_row), status, font=font_sm, fill=col_text)
            cx += col_w[3]
            # é‡è¦ãªç†ç”±
            why = e.get('why', '')[:20]
            g.text((cx, y_row), why, font=font_sm, fill=col_text)
            cx += col_w[4]
            y_row += 16
            if y_row > y0 + h - 10:
                break  # prevent overflow
    ev_box_x0 = left_col_x0
    ev_box_y0 = max(tis_y1, st_y1) + 40
    ev_box_w = W - padding - ev_box_x0
    ev_box_h = 140
    evidence = (extras or {}).get("evidence") if extras else None
    draw_evidence(ev_box_x0, ev_box_y0, ev_box_w, ev_box_h, evidence)

    # (moved overlay panel below after caption lines are drawn)

    # Caption (What / So what / Next action) â€” compact 3 lines with data provenance
    cap_y = ev_box_y0 + ev_box_h + 16
    # sprint meta
    sprint_label = (f"ã‚¹ãƒ—ãƒªãƒ³ãƒˆ {sprint_name}" if sprint_name else "ã‚¹ãƒ—ãƒªãƒ³ãƒˆ")
    d0 = fmt_date(sprint_start) if sprint_start else None
    d1 = fmt_date(sprint_end) if sprint_end else None
    if d0 and d1:
        sprint_label = f"{sprint_label} ({d0}-{d1})"
    # KPI numbers if available
    kpi_data = (extras or {}).get("kpis", {}) if extras else {}
    sprint_total = int(kpi_data.get("sprintTotal", 0))
    sprint_done = int(kpi_data.get("sprintDone", 0))
    # time-in-status Review avg (days)
    review_avg = None
    tis_obj = (extras or {}).get("time_in_status") if extras else None
    try:
        per_issue = (tis_obj or {}).get("perIssue") or []
        sum_map: Dict[str, float] = {}
        cnt_map: Dict[str, int] = {}
        for row in per_issue:
            by = row.get("byStatus") or {}
            for st, days in by.items():
                d = float(days) if days is not None else 0.0
                sum_map[st] = sum_map.get(st, 0.0) + d
                cnt_map[st] = cnt_map.get(st, 0) + 1
        # find Review-like key
        if sum_map:
            # pick exact 'Review' else any containing 'Review'
            key_candidates = [k for k in sum_map.keys() if str(k).lower() == "review"] or [k for k in sum_map.keys() if "review" in str(k).lower()]
            if key_candidates:
                k0 = key_candidates[0]
                review_avg = sum_map[k0] / max(1, cnt_map.get(k0, 1))
    except Exception:
        pass
    # Build context for Gemini summary
    risks_data = (extras or {}).get("risks", {}) if extras else {}
    # Action recommendations based on data
    action_suggestions = []
    hp = int(risks_data.get("highPriorityTodo", 0))
    od = int(risks_data.get("overdue", 0))
    ds = int(risks_data.get("dueSoon", 0))
    if done_rate < target_done_rate:
        action_suggestions.append("ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‹…å½“ã®å¢—å“¡/ä¸¦åˆ—åŒ–ã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„")
    if hp > 0:
        action_suggestions.append(f"é«˜å„ªå…ˆåº¦æœªç€æ‰‹ {hp}ä»¶ã«å³æ™‚æ‹…å½“å‰²å½“")
    if od > 0:
        action_suggestions.append(f"æœŸé™è¶…éŽ {od}ä»¶ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    if ds > 0:
        action_suggestions.append(f"æœŸé™æŽ¥è¿‘ {ds}ä»¶ã®å„ªå…ˆé †ä½å†ç¢ºèª")
    
    # æ–°ã—ã„contextã‚­ãƒ¼ã®è¨ˆç®—
    try:
        kpi_data = (extras or {}).get("kpis", {}) if extras else {}
        project_open_total = int(kpi_data.get("projectOpenTotal", 0))
        sprint_open = int(kpi_data.get("sprintOpen", 0))  # ç›´æŽ¥sprintOpenã‚’ä½¿ç”¨
        backlog_open = max(0, project_open_total - sprint_open)
        
        # Velocityé–¢é€£ã®è¨ˆç®—
        velocity_data = (extras or {}).get("velocity") if extras else None
        velocity_avg = 0.0
        last_velocity = 0.0
        if velocity_data:
            if "avg" in velocity_data:  # æ–°å½¢å¼
                velocity_avg = float(velocity_data.get("avg", 0.0))
                history = velocity_data.get("history", [])
                if history:
                    last_velocity = float(history[0].get("points", 0.0))
            else:  # æ—§å½¢å¼
                velocity_avg = float(velocity_data.get("avgPoints", 0.0))
                points = velocity_data.get("points", [])
                if points:
                    last_velocity = float(points[0].get("points", 0.0))
        
        # æ®‹æ—¥æ•°ã®è¨ˆç®—ï¼ˆã‚¹ãƒ—ãƒªãƒ³ãƒˆçµ‚äº†æ—¥ã‹ã‚‰ï¼‰
        remaining_days = 0
        if sprint_end:
            try:
                from datetime import datetime, date
                if "T" in sprint_end:
                    end_date = datetime.fromisoformat(sprint_end.replace("Z", "+00:00")).date()
                else:
                    end_date = datetime.strptime(sprint_end, "%Y-%m-%d").date()
                today = date.today()
                remaining_days = max(0, (end_date - today).days)
            except Exception:
                remaining_days = 0
        
        # å¿…è¦ãªæ—¥æ¬¡æ¶ˆåŒ–æ•°ã®è¨ˆç®—
        required_daily_burn = None
        if remaining_days > 0:
            import math
            target_remaining = max(0, int(target_done_rate * sprint_total) - sprint_done)
            required_daily_burn = math.ceil(target_remaining / remaining_days) if target_remaining > 0 else 0
        
        # å®Ÿç¸¾æ—¥æ¬¡æ¶ˆåŒ–æ•°ï¼ˆç›´è¿‘3æ—¥ã®å¹³å‡ï¼‰
        actual_daily_burn = None
        burndown_data = (extras or {}).get("burndown") if extras else None
        if burndown_data:
            time_series = burndown_data.get("timeSeries", [])
            if len(time_series) >= 4:  # æœ€ä½Ž4æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                try:
                    burn_lookback_days = int(os.getenv("BURN_LOOKBACK_DAYS", "3"))
                    recent_series = time_series[-burn_lookback_days-1:]  # æœ€æ–°N+1æ—¥åˆ†
                    if len(recent_series) >= 2:
                        total_burned = 0.0
                        for i in range(len(recent_series) - 1):
                            prev_remaining = float(recent_series[i].get("remaining", 0.0))
                            curr_remaining = float(recent_series[i+1].get("remaining", 0.0))
                            daily_burn = max(0.0, prev_remaining - curr_remaining)
                            total_burned += daily_burn
                        actual_daily_burn = total_burned / max(1, len(recent_series) - 1)
                except Exception:
                    actual_daily_burn = None
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å·¥ç¨‹ã®ç‰¹å®š
        bottleneck_status = None
        bottleneck_days = 0.0
        tis_data = (extras or {}).get("time_in_status") if extras else None
        if tis_data:
            per_issue = tis_data.get("perIssue", [])
            if per_issue:
                # å„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®å¹³å‡æ»žåœ¨æ™‚é–“ã‚’è¨ˆç®—
                status_totals = {}
                status_counts = {}
                for issue in per_issue:
                    by_status = issue.get("byStatus", {})
                    for status, days in by_status.items():
                        try:
                            days_float = float(days)
                            status_totals[status] = status_totals.get(status, 0.0) + days_float
                            status_counts[status] = status_counts.get(status, 0) + 1
                        except Exception:
                            continue
                
                # æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç‰¹å®š
                max_avg_days = 0.0
                for status in status_totals:
                    avg_days = status_totals[status] / max(1, status_counts[status])
                    if avg_days > max_avg_days:
                        max_avg_days = avg_days
                        bottleneck_status = status
                        bottleneck_days = avg_days
        
    except Exception:
        project_open_total = 0
        sprint_open = sprint_total - sprint_done
        backlog_open = 0
        velocity_avg = 0.0
        last_velocity = 0.0
        remaining_days = 0
        required_daily_burn = None
        actual_daily_burn = None
        bottleneck_status = None
        bottleneck_days = 0.0

    context_for_ai = {
        "sprint_label": sprint_label,
        "sprint_total": sprint_total,
        "sprint_done": sprint_done,
        "done_percent": round(done_rate * 100, 1),
        "target_percent": int(target_done_rate * 100),
        "remaining_days": remaining_days,
        "required_daily_burn": required_daily_burn,
        "actual_daily_burn": actual_daily_burn,
        "sprint_open": sprint_open,
        "backlog_open": backlog_open,
        "velocity_avg": velocity_avg,
        "last_velocity": last_velocity,
        "bottleneck_status": bottleneck_status,
        "bottleneck_days": bottleneck_days,
        "review_avg_days": review_avg,
        "overdue": int(risks_data.get("overdue", 0)),
        "due_soon": int(risks_data.get("dueSoon", 0)),
        "high_priority_unstarted": int(risks_data.get("highPriorityTodo", 0)),
        "suggested_actions": action_suggestions,
        "top_evidence": (extras or {}).get("evidence", []) or [],
        "project_open_total": project_open_total,
    }
    raw_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    gemini_key = _sanitize_api_key(raw_key)
    # Gemini diagnostics
    _log_on = (os.getenv("DASHBOARD_LOG", "1").lower() in ("1", "true", "yes"))
    _gemini_disabled = os.getenv("GEMINI_DISABLE", "").lower() in ("1", "true", "yes")
    ai = None
    if gemini_key and not _gemini_disabled and genai is not None:
        if GEMINI_DEBUG and _log_on:
            masked = f"{gemini_key[:4]}...{gemini_key[-4:]}" if len(gemini_key) >= 8 else "(set)"
            if raw_key and raw_key != gemini_key:
                print("- AIè¦ç´„: APIã‚­ãƒ¼ã‚’æ­£è¦åŒ–ã—ã¾ã—ãŸï¼ˆéžASCIIã‚„ä½™åˆ†ãªè¨˜å·ã‚’é™¤åŽ»ï¼‰")
            print(f"- AIè¦ç´„: ã‚­ãƒ¼æ¤œå‡º {masked}")
        ai = maybe_gemini_summary(gemini_key, context_for_ai)
        if _log_on:
            if ai:
                print("- AIè¦ç´„: Gemini æˆåŠŸ (å…¨æ–‡å–å¾—)")
            else:
                print("- AIè¦ç´„: Gemini å‘¼ã³å‡ºã—å¤±æ•—ã¾ãŸã¯ç©ºå¿œç­”")
        try:
            if isinstance(extras, dict):
                extras["ai_full_text"] = ai if isinstance(ai, str) else None
        except Exception:
            pass
    else:
        if _log_on:
            if _gemini_disabled:
                print("- AIè¦ç´„: ç„¡åŠ¹åŒ– (GEMINI_DISABLE)")
            elif not gemini_key:
                print("- AIè¦ç´„: æœªè¨­å®š (GEMINI_API_KEY/GOOGLE_API_KEY ãªã—)")
            else:
                print("- AIè¦ç´„: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªå°Žå…¥ (google-generativeai)")
    # Image caption remains deterministic and data-driven; AI full text goes to markdown
    what = f"What: {sprint_label} â€” å°ã‚¿ã‚¹ã‚¯ {total_cnt}ä»¶, å®Œäº† {done_cnt} ({int((done_cnt/max(1,total_cnt))*100)}%). (data: sprint_subtasks_total={total_cnt}, sprint_subtasks_done={done_cnt})"
    if done_rate < target_done_rate:
        if review_avg is not None:
            sowhat = f"So what: ç›®æ¨™{int(target_done_rate*100)}%æœªé”ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼æ»žç•™ (data: time_in_status[Review].avg={review_avg:.1f}d)"
        else:
            sowhat = f"So what: ç›®æ¨™{int(target_done_rate*100)}%æœªé”"
    else:
        sowhat = "So what: ãƒ™ãƒ­ã‚·ãƒ†ã‚£å®‰å®šã€è¨ˆç”»é€šã‚Š"
    hp = int(risks_data.get("highPriorityTodo", 0))
    nexta = f"Next: é«˜å„ªå…ˆåº¦æœªå®Œäº†{hp}ä»¶ã®å‰²å½“ã¨ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‹…å½“å¢—å“¡"
    g.text((proj_x0, cap_y), what, font=font_sm, fill=col_text)
    g.text((proj_x0, cap_y + 16), sowhat, font=font_sm, fill=col_text)
    g.text((proj_x0, cap_y + 32), nexta, font=font_sm, fill=col_text)

    # AI summary overlay panel (wrapped text in image) â€” runs after caption to avoid NameError
    try:
        overlay_enabled = AI_OVERLAY_IN_IMAGE
        ai_text = (extras or {}).get("ai_full_text") if extras else None
        if overlay_enabled and isinstance(ai_text, str) and ai_text.strip():
            # Keep the full AI summary content without truncation
            try:
                import re as _re
                # Remove excessive whitespace but keep all content
                ai_text = _re.sub(r"\r", "", ai_text)
                ai_text = _re.sub(r"\n[ \t]*\n+", "\n", ai_text)
            except Exception:
                pass
            panel_x0 = proj_x0
            panel_w = W - padding - panel_x0
            # Place directly under evidence to guarantee space
            panel_y0 = ev_box_y0 + ev_box_h + 12
            footer_reserve = 28
            panel_h = max(72, H - padding - footer_reserve - panel_y0)
            if panel_h >= 24:
                g.rectangle([panel_x0, panel_y0, panel_x0 + panel_w, panel_y0 + panel_h], outline=col_outline, fill=(245, 245, 245))
                title = "AIè¦ç´„ (Gemini)"
                g.text((panel_x0 + 8, panel_y0 + 6), title, font=font_md, fill=col_text)

                def wrap_text(text: str, max_width: int, font: ImageFont.ImageFont) -> List[str]:
                    lines: List[str] = []
                    for raw in text.split("\n"):
                        s = raw.rstrip("\r")
                        if not s:
                            lines.append("")
                            continue
                        buf = ""
                        for ch in s:
                            cand = buf + ch
                            try:
                                # çµµæ–‡å­—ã‚„ç‰¹æ®Šæ–‡å­—ã®æç”»å¹…ã‚’å®‰å…¨ã«è¨ˆç®—
                                if g.textlength(cand, font=font) <= max_width:
                                    buf = cand
                                else:
                                    if buf:
                                        lines.append(buf)
                                        buf = ch
                                    else:
                                        lines.append(ch)
                                        buf = ""
                            except Exception:
                                # æ–‡å­—å¹…è¨ˆç®—ã«å¤±æ•—ã—ãŸå ´åˆã¯å®‰å…¨ã«å‡¦ç†
                                if len(buf) > 0:
                                    lines.append(buf)
                                    buf = ch
                                else:
                                    buf = ch
                        if buf != "":
                            lines.append(buf)
                    return lines



                content_x = panel_x0 + 8
                content_y = panel_y0 + 6 + text_wh(title, font_md)[1] + 4
                content_w = panel_w - 16
                content_font = font_sm
                line_h = max(14, text_wh("A", content_font)[1])
                max_lines_by_height = max(1, (panel_h - (content_y - panel_y0) - 8) // line_h)
                try:
                    max_lines_cap = int(AI_OVERLAY_MAX_LINES)
                except Exception:
                    max_lines_cap = 18
                max_lines = max(1, min(max_lines_by_height, max_lines_cap))
                total_wrapped = wrap_text(ai_text.strip(), content_w, content_font)
                # Draw within one image; truncate with ellipsis if overflow
                y = content_y
                shown = 0
                for ln in total_wrapped:
                    if shown + 1 < max_lines:
                        g.text((content_x, y), ln, font=content_font, fill=col_text)
                        y += line_h
                        shown += 1
                    else:
                        # last visible line with ellipsis
                        last = ln
                        ell = "â€¦"
                        while last and g.textlength(last + ell, font=content_font) > content_w:
                            last = last[:-1]
                        g.text((content_x, y), (last + ell) if last else "â€¦", font=content_font, fill=col_text)
                        break
    except Exception:
        pass

    # Footer timestamp (bottom-right)
    try:
        import datetime as _dt
        ts = _dt.datetime.now().strftime("ç”Ÿæˆ: %Y/%m/%d %H:%M")
        tw = g.textlength(ts, font=font_sm)
        g.text((W - padding - tw, H - padding - getattr(font_sm, "size", 12)), ts, font=font_sm, fill=(120, 120, 120))
    except Exception:
        pass

    img.save(output_path, format="PNG", dpi=(150, 150))


def main() -> int:
    ensure_env_loaded()
    JIRA_DOMAIN = os.getenv("JIRA_DOMAIN", "").rstrip("/")
    email = os.getenv("JIRA_EMAIL")
    api_token = os.getenv("JIRA_API_TOKEN")
    boards_n = 1
    sprints_n = 1
    sprint_name = None
    sprint_start = None
    sprint_end = None
    board = None
    sprint = None
    auth: Optional[HTTPBasicAuth] = None
    if JIRA_DOMAIN and email and api_token:
        auth = HTTPBasicAuth(email, api_token)
        code_b, board, _ = resolve_board(JIRA_DOMAIN, auth)
        if code_b == 200 and board:
            proj_key = os.getenv("JIRA_PROJECT_KEY") or try_infer_project_key_from_board(JIRA_DOMAIN, auth, board) or None
            boards_n = max(1, count_boards_for_project(JIRA_DOMAIN, auth, proj_key))
            try:
                bid = int(board.get("id"))
                sprints_n = max(1, count_active_sprints_for_board(JIRA_DOMAIN, auth, bid))
                sprint = resolve_active_sprint(JIRA_DOMAIN, auth, bid)
                if sprint:
                    sprint_name = sprint.get("name")
                    sprint_start = sprint.get("startDate")
                    sprint_end = sprint.get("endDate")
            except Exception:
                sprints_n = 1
    base_dir = Path(os.getenv("OUTPUT_DIR") or Path(__file__).resolve().parent)
    subtasks_script = str(base_dir / "queries" / "jira_list_sprint_subtasks.py")
    data = get_json_from_script(subtasks_script)
    out_path = str(base_dir / "sprint_overview.png")
    axis_mode = os.getenv("AXIS_MODE", "percent").lower()  # 'percent' or 'count'
    try:
        target_done_rate = float(os.getenv("TARGET_DONE_RATE", "0.8"))
    except Exception:
        target_done_rate = 0.8
    # Fetch extra metrics for dashboard
    extras: Dict[str, Any] = {}
    try:
        # A. Burndown
        bd_args = ["--unit", BURNDOWN_UNIT]
        extras["burndown"] = get_json_from_script_args(str(base_dir / "queries" / "jira_q_burndown.py"), bd_args)
    except Exception:
        extras["burndown"] = None
    try:
        # B. Velocity
        vel_args: List[str] = []
        n_sprints = os.getenv("N_SPRINTS", "6")
        if n_sprints:
            vel_args += ["--n", n_sprints]
        extras["velocity"] = get_json_from_script_args(str(base_dir / "queries" / "jira_q_velocity_history.py"), vel_args)
    except Exception as e:
        print(f"Error getting velocity data: {e}")
        extras["velocity"] = None
    try:
        # B2. Project sprint count (all states)
        extras["project_sprint_count"] = get_json_from_script_args(str(base_dir / "queries" / "jira_count_project_sprints.py"), [])
    except Exception:
        extras["project_sprint_count"] = None
    try:
        # C. Status distribution (sprint scope, approx)
        sc_args = ["--scope", "sprint", "--mode", os.getenv("STATUS_COUNTS_MODE", "approx")]
        extras["status_counts"] = get_json_from_script_args(str(base_dir / "queries" / "jira_q_status_counts.py"), sc_args)
    except Exception:
        extras["status_counts"] = None
    try:
        # D. Time-in-Status (for evidence calc; unit days)
        tis_args = ["--scope", "sprint", "--unit", os.getenv("TIS_UNIT", "days")]
        tis = get_json_from_script_args(str(base_dir / "queries" / "jira_q_time_in_status.py"), tis_args)
        extras["time_in_status"] = tis
    except Exception:
        extras["time_in_status"] = None
    try:
        # E. Assignee workload
        wl_args = ["--scope", "sprint"]
        extras["workload"] = get_json_from_script_args(str(base_dir / "queries" / "jira_q_assignee_workload.py"), wl_args)
    except Exception:
        extras["workload"] = None
    # F. KPI cards and risks
    kpis: Dict[str, int] = {"projectTotal": 0, "sprintTotal": 0, "sprintOpen": 0, "sprintDone": 0, "unassignedCount": 0, "overdue": 0, "dueSoon": 0, "highPriorityTodo": 0}
    risks: Dict[str, int] = {"overdue": 0, "dueSoon": 0, "highPriorityTodo": 0}
    try:
        od_args = ["--scope", "sprint"]
        overdue = get_json_from_script_args(str(base_dir / "queries" / "jira_q_overdue_count.py"), od_args)
        risks["overdue"] = int(overdue.get("overdueCount") or 0)
        kpis["overdue"] = risks["overdue"]
    except Exception:
        pass
    try:
        ds_days = os.getenv("DUE_SOON_DAYS", "7")
        ds_args = ["--scope", "sprint", "--days", ds_days]
        due_soon = get_json_from_script_args(str(base_dir / "queries" / "jira_q_due_soon_count.py"), ds_args)
        risks["dueSoon"] = int(due_soon.get("dueSoonCount") or 0)
    except Exception:
        pass
    try:
        # High priority unstarted count
        if auth and sprint and JIRA_DOMAIN:
            sid = sprint.get("id")
            pri = os.getenv("HIGH_PRIORITIES", "Highest,High")
            pri_list = ",".join([f'"{p.strip()}"' for p in pri.split(",") if p.strip()])
            jql = f"Sprint={sid} AND type in subTaskIssueTypes() AND priority in ({pri_list}) AND statusCategory = \"To Do\""
            code_c, cnt, _ = approximate_count(JIRA_DOMAIN, auth, jql)
            if not (code_c == 200 and cnt is not None):
                code_c, cnt, _ = search_count(JIRA_DOMAIN, auth, jql)
            if code_c == 200 and cnt is not None:
                risks["highPriorityTodo"] = int(cnt)
    except Exception:
        pass
    # Unassigned count
    try:
        unassigned_data = get_json_from_script_args(str(base_dir / "queries" / "jira_q_unassigned_count.py"), ["--scope", "sprint"])
        if isinstance(unassigned_data, dict):
            kpis["unassignedCount"] = int(unassigned_data.get("unassignedCount", 0))
    except Exception:
        pass
    # Enforce risks to be subtask-based (override with subtask-only JQL when possible)
    try:
        if auth and sprint and JIRA_DOMAIN:
            sid = sprint.get("id")
            # Overdue (subtasks only)
            jql_od = f"Sprint={sid} AND type in subTaskIssueTypes() AND duedate < endOfDay() AND statusCategory != \"Done\""
            code_od, cnt_od, _ = approximate_count(JIRA_DOMAIN, auth, jql_od)
            if not (code_od == 200 and cnt_od is not None):
                code_od, cnt_od, _ = search_count(JIRA_DOMAIN, auth, jql_od)
            if code_od == 200 and cnt_od is not None:
                risks["overdue"] = int(cnt_od)
                kpis["overdue"] = risks["overdue"]
            # Due soon (subtasks only)
            ds_days = os.getenv("DUE_SOON_DAYS", "7")
            jql_ds = (
                f"Sprint={sid} AND type in subTaskIssueTypes() "
                f"AND duedate >= startOfDay() AND duedate <= endOfDay(+{ds_days}d) "
                f"AND statusCategory != \"Done\""
            )
            code_ds, cnt_ds, _ = approximate_count(JIRA_DOMAIN, auth, jql_ds)
            if not (code_ds == 200 and cnt_ds is not None):
                code_ds, cnt_ds, _ = search_count(JIRA_DOMAIN, auth, jql_ds)
            if code_ds == 200 and cnt_ds is not None:
                risks["dueSoon"] = int(cnt_ds)
    except Exception:
        pass
    # projectTotal / sprintTotal counts (subtask-based for sprint)
    try:
        # Sprint totals from aggregated data (subtasks only)
        try:
            totals_obj = (data or {}).get("totals", {}) if isinstance(data, dict) else {}
            kpis["sprintTotal"] = int(totals_obj.get("subtasks", 0))
            kpis["sprintDone"] = int(totals_obj.get("done", 0))
            kpis["sprintOpen"] = int(totals_obj.get("notDone", 0))  # æœªå®Œäº†ã‚¿ã‚¹ã‚¯æ•°
        except Exception:
            pass
        if auth and JIRA_DOMAIN:
            # Prefer dedicated query script for project subtasks
            try:
                ps = get_json_from_script_args(str(base_dir / "queries" / "jira_count_project_subtasks.py"), [])
                if isinstance(ps, dict):
                    extras["project_subtask_count"] = ps
                    kpis["projectTotal"] = int(ps.get("total", 0))
                    kpis["projectOpenTotal"] = int(ps.get("openTotal", 0))  # æœªå®Œäº†ã‚¿ã‚¹ã‚¯æ•°
                    # projectAllSubtasksã‚’ä¿æŒï¼ˆã‚°ãƒ©ãƒ•éžè¡¨ç¤ºã€ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
                    extras["projectAllSubtasks"] = int(ps.get("total", 0))
            except Exception:
                # Fallback to inline JQL if script fails
                proj_key = os.getenv("JIRA_PROJECT_KEY") or try_infer_project_key_from_board(JIRA_DOMAIN, auth, board) or None
                if proj_key:
                    jql_proj_sub = f"project={proj_key} AND type in subTaskIssueTypes()"
                    code_pt, cnt_pt, _ = approximate_count(JIRA_DOMAIN, auth, jql_proj_sub)
                    if not (code_pt == 200 and cnt_pt is not None):
                        code_pt, cnt_pt, _ = search_count(JIRA_DOMAIN, auth, jql_proj_sub)
                    if code_pt == 200 and cnt_pt is not None:
                        kpis["projectTotal"] = int(cnt_pt)
                        extras["projectAllSubtasks"] = int(cnt_pt)  # å…¨ã‚¿ã‚¹ã‚¯æ•°ä¿æŒ
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœªå®Œäº†æ•°ã‚‚å–å¾—
                        jql_proj_open = f"project={proj_key} AND type in subTaskIssueTypes() AND statusCategory != \"Done\""
                        code_po, cnt_po, _ = approximate_count(JIRA_DOMAIN, auth, jql_proj_open)
                        if not (code_po == 200 and cnt_po is not None):
                            code_po, cnt_po, _ = search_count(JIRA_DOMAIN, auth, jql_proj_open)
                        if code_po == 200 and cnt_po is not None:
                            kpis["projectOpenTotal"] = int(cnt_po)
    except Exception:
        pass
    # carry risks into KPI deck as well
    kpis["overdue"] = max(kpis.get("overdue", 0), risks.get("overdue", 0))
    kpis["dueSoon"] = risks.get("dueSoon", 0)
    kpis["highPriorityTodo"] = risks.get("highPriorityTodo", 0)
    extras["kpis"] = kpis
    extras["risks"] = risks

    # G. Evidence table: Top N by longest time-in-status (days)
    try:
        ev_list: List[Dict[str, Any]] = []
        tis = extras.get("time_in_status") or {}
        unit = ((tis.get("window") or {}).get("unit") or "days")
        denom = 1.0  # already in days
        per_issue = tis.get("perIssue") or []
        for row in per_issue:
            key = row.get("key")
            by = row.get("byStatus") or {}
            days = sum(float(v) for v in by.values()) / (1.0 if unit == "days" else 24.0)
            ev_list.append({"key": key, "days": days})
        # sort and take top N
        ev_list = [e for e in ev_list if e.get("key")]
        ev_list.sort(key=lambda r: -float(r.get("days") or 0.0))
        topn = int(os.getenv("EVIDENCE_TOP_N", "5"))
        ev_list = ev_list[:topn]
        # fetch current status for these keys
        if auth and JIRA_DOMAIN and ev_list:
            keys_csv = ",".join([str(e["key"]) for e in ev_list])
            fields = "summary,status,assignee,priority,duedate"
            fields_list = [f.strip() for f in fields.split(",")]
            code_s, data_s, _ = search_jql_page(
                JIRA_DOMAIN,
                auth,
                f"key in ({keys_csv})",
                fields_list,
                max_results=max(1, topn),
            )
            detail_map: Dict[str, Dict[str, Any]] = {}
            if code_s == 200 and data_s:
                for iss in (data_s.get("issues") or []):
                    flds = iss.get("fields") or {}
                    detail_map[iss.get("key")] = {
                        "summary": flds.get("summary") or "",
                        "status": ((flds.get("status") or {}).get("name") or ""),
                        "assignee": ((flds.get("assignee") or {}).get("displayName") or ""),
                        "priority": ((flds.get("priority") or {}).get("name") or ""),
                        "duedate": flds.get("duedate") or "",
                    }
            # attach status, assignee, why and link
            dom = JIRA_DOMAIN.rstrip("/")
            for e in ev_list:
                k = e.get("key")
                det = detail_map.get(k, {})
                e["summary"] = det.get("summary", "")
                e["status"] = det.get("status", "")
                e["assignee"] = det.get("assignee", "")
                # why heuristic
                why = []
                try:
                    dd = det.get("duedate")
                    if dd:
                        # overdue if past today
                        import datetime as _dt
                        today = _dt.date.today()
                        ddd = _dt.datetime.strptime(dd, "%Y-%m-%d").date()
                        if ddd < today:
                            why.append("overdue")
                        elif (ddd - today).days <= int(os.getenv("DUE_SOON_DAYS", "7")):
                            why.append("due soon")
                except Exception:
                    pass
                pr = str(det.get("priority", ""))
                if pr.lower() in [p.strip().lower() for p in (os.getenv("HIGH_PRIORITIES", "Highest,High").split(","))]:
                    why.append("high priority")
                if float(e.get("days") or 0) >= 5.0:
                    why.append("long stay")
                e["why"] = ", ".join(why)
                e["link"] = f"{dom}/browse/{k}"
        # Optionally enhance 'why' with Gemini (one-liner) while keeping safe fallback
        try:
            raw_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
            gemini_key = _sanitize_api_key(raw_key)
            if gemini_key and ev_list:
                ai_reasons = maybe_gemini_justify_evidences(gemini_key, ev_list)
                if ai_reasons:
                    for e in ev_list:
                        k = e.get("key")
                        if k and ai_reasons.get(k):
                            e["why"] = ai_reasons[k]
        except Exception:
            pass
        extras["evidence"] = ev_list
    except Exception:
        extras["evidence"] = None

    # --- Logging (concise summaries) ---
    try:
        log_enabled = (os.getenv("DASHBOARD_LOG", "1").lower() in ("1", "true", "yes"))
        if log_enabled:
            print("[Dashboard] ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚µãƒžãƒªãƒ¼")
            try:
                totals = data.get("totals", {}) if isinstance(data, dict) else {}
                print(f"- å°ã‚¿ã‚¹ã‚¯: åˆè¨ˆ {int(totals.get('subtasks', 0))}, å®Œäº† {int(totals.get('done', 0))}, æœªå®Œäº† {int(totals.get('subtasks', 0)) - int(totals.get('done', 0))}")
            except Exception:
                pass
            # Burndown
            bd = extras.get("burndown")
            if isinstance(bd, dict):
                try:
                    ts = bd.get("timeSeries") or []
                    first = float((ts[0] or {}).get("remaining", 0.0)) if ts else 0.0
                    last = float((ts[-1] or {}).get("remaining", 0.0)) if ts else 0.0
                    print(f"- Burndown: unit={bd.get('unit')}, total={bd.get('total')}, days={len(ts)}, æ®‹(å§‹â†’çµ‚) {first:.1f}â†’{last:.1f}")
                except Exception:
                    print("- Burndown: å–å¾—å¤±æ•—ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—")
            else:
                print("- Burndown: ãƒ‡ãƒ¼ã‚¿ãªã—")
            # Velocity
            vel = extras.get("velocity")
            if isinstance(vel, dict):
                try:
                    # Handle new velocity format directly for display
                    if "history" in vel:
                        # New format: convert for display
                        history = vel.get("history", [])
                        avg_points = vel.get("avg", 0.0)
                        tail = [float(h.get("points", 0.0)) for h in history[-3:]]
                        print(f"- Velocity: avg={avg_points}, sprints={len(history)}, last={tail}")
                    else:
                        # Old format: use directly
                        pts = vel.get("points") or []
                        tail = [float(p.get("points") or 0.0) for p in pts[-3:]]
                        print(f"- Velocity: avg={vel.get('avgPoints')}, sprints={len(pts)}, last={tail}")
                except Exception as e:
                    print(f"- Velocity: å–å¾—å¤±æ•—ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã— ({e})")
            else:
                print("- Velocity: ãƒ‡ãƒ¼ã‚¿ãªã—")
            # Status counts
            sc = extras.get("status_counts")
            if isinstance(sc, dict):
                try:
                    total = int(sc.get("total", 0))
                    bys = sc.get("byStatus") or []
                    print(f"- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ†å¸ƒ: total={total}, ç¨®é¡ž={len(bys)}")
                except Exception:
                    print("- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ†å¸ƒ: å–å¾—å¤±æ•—")
            else:
                print("- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ†å¸ƒ: ãƒ‡ãƒ¼ã‚¿ãªã—")
            # Time in Status
            tis = extras.get("time_in_status")
            if isinstance(tis, dict):
                try:
                    per_issue = tis.get("perIssue") or []
                    print(f"- å·¥ç¨‹æ»žåœ¨æ™‚é–“: issues={len(per_issue)}")
                except Exception:
                    print("- å·¥ç¨‹æ»žåœ¨æ™‚é–“: å–å¾—å¤±æ•—")
            else:
                print("- å·¥ç¨‹æ»žåœ¨æ™‚é–“: ãƒ‡ãƒ¼ã‚¿ãªã—")
            # Workload
            wl = extras.get("workload")
            if isinstance(wl, dict):
                try:
                    rows = wl.get("byAssignee") or []
                    names = [str(r.get("name")) for r in rows[:3]]
                    print(f"- ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰: æ‹…å½“è€…={len(rows)}, top3={names}")
                except Exception:
                    print("- ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰: å–å¾—å¤±æ•—")
            else:
                print("- ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰: ãƒ‡ãƒ¼ã‚¿ãªã—")
            # KPIs / Risks
            k = extras.get("kpis") or {}
            r = extras.get("risks") or {}
            try:
                print(f"- KPI: projectTotal={k.get('projectTotal')}, sprintTotal={k.get('sprintTotal')}, sprintOpen={k.get('sprintOpen')}, unassignedCount={k.get('unassignedCount')}")
                print(f"- ãƒªã‚¹ã‚¯: overdue={r.get('overdue')}, dueSoon={r.get('dueSoon')}, highPriorityTodo={r.get('highPriorityTodo')}")
            except Exception:
                pass
            # Project subtasks (detailed)
            psc2 = extras.get("project_subtask_count")
            if isinstance(psc2, dict):
                try:
                    print(f"- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ(å°ã‚¿ã‚¹ã‚¯): total={psc2.get('total')}, done={psc2.get('done')}, notDone={psc2.get('notDone')}")
                except Exception:
                    pass
            # Project sprint counts
            psc = extras.get("project_sprint_count")
            if isinstance(psc, dict):
                try:
                    bys = psc.get("byState", {}) or {}
                    print(f"- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¹ãƒ—ãƒªãƒ³ãƒˆæ•°: total={psc.get('total')} (active={bys.get('active')}, future={bys.get('future')}, closed={bys.get('closed')})")
                except Exception:
                    pass
            # Evidence
            ev = extras.get("evidence")
            if isinstance(ev, list):
                try:
                    keys = [e.get("key") for e in ev[:5] if e.get("key")]
                    print(f"- ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹TopN: {len(ev)} ä»¶, keys={keys}")
                except Exception:
                    print("- ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹: å–å¾—å¤±æ•—")
            elif ev is None:
                print("- ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹: ãƒ‡ãƒ¼ã‚¿ãªã—")
    except Exception:
        # ãƒ­ã‚°ã¯å¤±æ•—ã—ã¦ã‚‚ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆã‚’ç¶™ç¶š
        pass

    draw_png(out_path, data, boards_n, sprints_n, sprint_name, sprint_start, sprint_end, axis_mode, target_done_rate, extras)
    # Also emit a concise Markdown report with evidence and risks
    try:
        ts = __import__("datetime").datetime.now().strftime("%Y-%m-%d %H:%M")
        kpis = extras.get("kpis", {})
        risks = extras.get("risks", {})
        tis = extras.get("time_in_status", {}) or {}
        # compute review avg again for text
        review_avg = None
        try:
            per_issue = (tis or {}).get("perIssue") or []
            sum_map: Dict[str, float] = {}
            cnt_map: Dict[str, int] = {}
            for row in per_issue:
                by = row.get("byStatus") or {}
                for st, days in by.items():
                    d = float(days) if days is not None else 0.0
                    sum_map[st] = sum_map.get(st, 0.0) + d
                    cnt_map[st] = cnt_map.get(st, 0) + 1
            if sum_map:
                key_candidates = [k for k in sum_map.keys() if str(k).lower() == "review"] or [k for k in sum_map.keys() if "review" in str(k).lower()]
                if key_candidates:
                    k0 = key_candidates[0]
                    review_avg = sum_map[k0] / max(1, cnt_map.get(k0, 1))
        except Exception:
            pass
        sprint_label = sprint.get("name") if sprint else "Sprint"
        if sprint and sprint.get("startDate") and sprint.get("endDate"):
            sprint_label = f"{sprint_label} ({sprint.get('startDate')}â€”{sprint.get('endDate')})"
        sprint_total = int(kpis.get("sprintTotal", 0))
        sprint_done = int(kpis.get("sprintDone", 0))
        target_pct = int(target_done_rate * 100)
        overdue_cnt = int(risks.get("overdue", 0))
        due_soon_cnt = int(risks.get("dueSoon", 0))
        hp_cnt = int(risks.get("highPriorityTodo", 0))
        # Risk keys
        overdue_keys: List[str] = []
        due_soon_keys: List[str] = []
        hp_keys: List[str] = []
        if auth and JIRA_DOMAIN and sprint:
            sid = sprint.get("id")
            if overdue_cnt:
                overdue_keys = search_issue_keys(
                    JIRA_DOMAIN,
                    auth,
                    f"Sprint={sid} AND type in subTaskIssueTypes() AND duedate < endOfDay() AND statusCategory != \"Done\"",
                    10,
                )
            if due_soon_cnt:
                days = os.getenv("DUE_SOON_DAYS", "7")
                due_soon_keys = search_issue_keys(
                    JIRA_DOMAIN,
                    auth,
                    f"Sprint={sid} AND type in subTaskIssueTypes() AND duedate >= startOfDay() AND duedate <= endOfDay(+{days}d) AND statusCategory != \"Done\"",
                    10,
                )
            if hp_cnt:
                pri = os.getenv("HIGH_PRIORITIES", "Highest,High")
                pri_list = ",".join([f'"{p.strip()}"' for p in pri.split(",") if p.strip()])
                hp_keys = search_issue_keys(
                    JIRA_DOMAIN,
                    auth,
                    f"Sprint={sid} AND type in subTaskIssueTypes() AND priority in ({pri_list}) AND statusCategory = \"To Do\"",
                    10,
                )
        # Evidence topN
        ev_rows = extras.get("evidence", []) or []
        # Markdown compose
        md = []
        md.append(f"## è¦ç´„ | {ts}")
        md.append(f"What: {sprint_label} â€” {sprint_total} tasks, Done {sprint_done} ({int((sprint_done/max(1,sprint_total))*100)}%). (data: sprint_total={sprint_total}, sprint_done={sprint_done})")
        if (sprint_done / max(1, sprint_total)) < target_done_rate:
            if review_avg is not None:
                md.append(f"So what: ç›®æ¨™{target_done_rate*100}%æœªé”ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼æ»žç•™ (data: time_in_status[Review].avg={review_avg:.1f}d)")
            else:
                md.append(f"So what: ç›®æ¨™{target_done_rate*100}%æœªé”")
        else:
            md.append("So what: ç›®æ¨™é”æˆãƒšãƒ¼ã‚¹")
        md.append(f"Next: é«˜å„ªå…ˆåº¦æœªå®Œäº†{hp_cnt}ä»¶ã®å³æ™‚å‰²å½“ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‹…å½“ã®å¢—å“¡æ¤œè¨Ž")
        md.append("")
        # If AI full summary exists, append as-is under its own section
        _ai_full = extras.get("ai_full_text") if isinstance(extras, dict) else None
        if isinstance(_ai_full, str) and _ai_full.strip():
            md.append("## AIè¦ç´„ (Gemini)")
            md.append("")
            md.append(_ai_full.strip())
            md.append("")
        md.append("## ãƒªã‚¹ã‚¯")
        if overdue_cnt:
            md.append(f"- æœŸé™è¶…éŽ: {overdue_cnt}ä»¶ ({', '.join(overdue_keys)}) â€” å„ªå…ˆå‰²å½“è¦")
        if due_soon_cnt:
            md.append(f"- 7æ—¥ä»¥å†…æœŸé™: {due_soon_cnt}ä»¶ ({', '.join(due_soon_keys)})")
        if hp_cnt:
            md.append(f"- é«˜å„ªå…ˆåº¦æœªç€æ‰‹: {hp_cnt}ä»¶ ({', '.join(hp_keys)})")
        if not (overdue_cnt or due_soon_cnt or hp_cnt):
            md.append("- ç‰¹ç­†ã™ã¹ããƒªã‚¹ã‚¯ãªã—")
        md.append("")
        md.append("## ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ (Top)")
        for e in ev_rows:
            _k = str(e.get('key') or '')
            _s = str(e.get('summary') or '').strip()
            if _s:
                ks = f"{_k} {_s}"
            else:
                ks = _k
            md.append(f"- {ks} | {e.get('status')} | {e.get('days'):.1f}d | assignee: {e.get('assignee','')} | why: {e.get('why','')} | {e.get('link')}")
        # Short actions
        md.append("")
        md.append("## çŸ­æœŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        md.append("1) ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‹…å½“ã‚’1åè¿½åŠ  â€” æœŸå¾…: Reviewå¹³å‡ã‚’2æ—¥çŸ­ç¸®")
        md.append("2) æœŸé™è¶…éŽã®å„ªå…ˆå‰²å½“ã¨ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
        with open(str(base_dir / "sprint_overview_report.md"), "w", encoding="utf-8") as f:
            f.write("\n".join(md))
    except Exception:
        pass
    # Write enriched tasks JSON for analysis
    try:
        enriched = {
            "sprint": {
                "name": sprint_name,
                "startDate": sprint_start,
                "endDate": sprint_end,
            },
            "parents": data.get("parents", []),
            "totals": data.get("totals", {}),
        }
        path_tasks = Path(base_dir) / "sprint_overview_tasks.json"
        with open(str(path_tasks), "w", encoding="utf-8") as f:
            json.dump(enriched, f, ensure_ascii=False, indent=2)
        if os.getenv("DASHBOARD_LOG", "1").lower() in ("1", "true", "yes"):
            print(str(path_tasks))
    except Exception as e:
        if os.getenv("DASHBOARD_LOG", "1").lower() in ("1", "true", "yes"):
            print(f"[warn] enriched tasks JSON ã®æ›¸ãå‡ºã—ã«å¤±æ•—: {e}")
    # Write metrics JSON for Slack integration
    try:
        totals = data.get("totals", {})
        done_cnt = int(totals.get("done", 0))
        total_cnt = int(totals.get("subtasks", 0))
        metrics = {
            "sprint": {
                "name": sprint_name,
                "startDate": sprint_start,
                "endDate": sprint_end,
            },
            "totals": totals,
            "doneRate": (done_cnt / total_cnt) if total_cnt else None,
            "targetDoneRate": target_done_rate,
            "axis": axis_mode,
            "extrasAvailable": {k: (v is not None) for k, v in (extras or {}).items()},
        }
        with open(str(base_dir / "sprint_overview_data.json"), "w", encoding="utf-8") as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
    except Exception:
        pass
    print(out_path)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
